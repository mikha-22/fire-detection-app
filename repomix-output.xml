This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
src/
  cloud_functions/
    pipeline_initiator/
      main.py
      requirements.txt
    result_processor/
      main.py
      requirements.txt
  common/
    config.py
  firms_data_retriever/
    retriever.py
  map_visualizer/
    visualizer.py
  ml_model/
    Dockerfile
    fire_detection_model.py
    handler.py
    register_vertex_model.py
    repomix-output.xml
    requirements.txt
  satellite_imagery_acquirer/
    acquirer.py
.gcloudignore
.gitignore
archive_model.sh
cloudbuild-pipeline-initiator.yaml
cloudbuild-result-processor.yaml
cloudbuild.yaml
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/cloud_functions/pipeline_initiator/requirements.txt">
# src/cloud_functions/pipeline_initiator/requirements.txt
requests
pandas
google-cloud-storage
google-cloud-aiplatform
earthengine-api # Required by SatelliteImageryAcquirer
# Pillow # Was removed in previous step, confirming it's still removed.
</file>

<file path="src/cloud_functions/result_processor/main.py">
# src/cloud_functions/result_processor/main.py

import os
import logging # Standard logging
import json
import io # For byte streams (map image)
import pandas as pd # For FIRMS data, though AI results are JSONL
from datetime import datetime, timedelta # Used for timestamps, date parsing
from google.cloud import storage # Correct GCS client
import base64 # Required for decoding Pub/Sub messages from Vertex AI
from typing import Dict, Any, List, Tuple, Optional # Good type hinting

# Import components from our project structure
from src.common.config import MONITORED_REGIONS, GCS_BUCKET_NAME
from src.firms_data_retriever.retriever import FirmsDataRetriever, FIRMS_API_BASE_URL, FIRMS_SENSORS
from src.map_visualizer.visualizer import MapVisualizer

# --- Configuration for Cloud Function ---
# Environment variables expected in Cloud Functions:
# GCP_PROJECT_ID: Your Google Cloud Project ID
# FIRMS_API_KEY: Your NASA FIRMS API key (TODO: Fetch from Secret Manager in Phase 3)
# GCP_REGION: (Not strictly used by this CF but good for consistency if other components need it)

# GCS paths for outputs, consistent with Pipeline Initiator
# GCS_BATCH_INPUT_DIR is not used here.
# GCS_BATCH_OUTPUT_DIR_PREFIX is where Vertex AI writes predictions.
GCS_FINAL_MAPS_DIR = "final_outputs/maps/" # Grouped final outputs
GCS_SOURCE_IMAGERY_COPY_DIR = "final_outputs/source_imagery/" # Optional: if we want to keep a copy of source images with final report
GCS_METADATA_DIR = "final_outputs/metadata/" # Grouped final outputs
FINAL_STATUS_FILENAME = "wildfire_status_latest.json" # More descriptive name

# --- Logging Setup ---
# logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s') # Can be removed
logger = logging.getLogger(__name__) # Standard practice

def _log_json(severity: str, message: str, **kwargs):
    """
    Helper to log structured JSON messages to stdout for GCP Cloud Logging.
    """
    log_entry = {
        "severity": severity.upper(),
        "message": message,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "component": "ResultProcessorCF",
        **kwargs
    }
    print(json.dumps(log_entry))


def _download_gcs_blob_as_bytes(storage_client: storage.Client, bucket_name: str, blob_name: str) -> Optional[bytes]:
    """Downloads a blob from GCS as bytes. Pass storage_client."""
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    try:
        if not blob.exists(): # Check existence before download attempt
            _log_json("WARNING", "GCS blob does not exist, cannot download.", bucket=bucket_name, blob_name=blob_name)
            return None
        contents = blob.download_as_bytes(timeout=30) # Add timeout
        _log_json("INFO", "Successfully downloaded GCS blob.", bucket=bucket_name, blob_name=blob_name, size_bytes=len(contents))
        return contents
    except Exception as e: # Catch any exception during download
        _log_json("ERROR", f"Failed to download GCS blob '{blob_name}' from bucket '{bucket_name}'.",
                   error=str(e), error_type=type(e).__name__, bucket=bucket_name, blob_name=blob_name)
        return None

def _upload_gcs_blob_from_bytes(storage_client: storage.Client, bucket_name: str, blob_name: str, data_bytes: bytes, content_type: str = "application/octet-stream"):
    """Uploads bytes to a GCS blob. Pass storage_client."""
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    try:
        blob.upload_from_string(data_bytes, content_type=content_type, timeout=60) # Add timeout
        _log_json("INFO", "Successfully uploaded GCS blob.", bucket=bucket_name, blob_name=blob_name, size_bytes=len(data_bytes))
    except Exception as e: # Catch any exception during upload
        _log_json("ERROR", f"Failed to upload GCS blob to '{blob_name}' in bucket '{bucket_name}'.",
                   error=str(e), error_type=type(e).__name__, bucket=bucket_name, blob_name=blob_name)
        raise # Re-raise to signal failure of this critical step

def _parse_vertex_ai_batch_output(storage_client: storage.Client, output_gcs_uri_prefix: str) -> Dict[str, Dict[str, Any]]:
    """
    Parses Vertex AI Batch Prediction output files (JSONL) from GCS.
    The output is in a directory, usually containing one or more prediction-*.jsonl files.
    
    Returns a dictionary mapping instance_id to its prediction result (merged instance + prediction).
    """
    prediction_results: Dict[str, Dict[str, Any]] = {}
    
    if not output_gcs_uri_prefix.startswith("gs://"):
        _log_json("ERROR", "Invalid GCS output URI prefix format. Must start with 'gs://'.", uri=output_gcs_uri_prefix)
        return {}
    
    path_parts = output_gcs_uri_prefix.replace("gs://", "").split("/", 1)
    if len(path_parts) < 2: # Should be at least bucket/path/
        _log_json("ERROR", "Invalid GCS output URI prefix, missing path component.", uri=output_gcs_uri_prefix)
        return {}

    bucket_name = path_parts[0]
    # The prefix from Vertex AI notification already points to the job's output directory.
    # Files are typically named like "prediction.results-00000-of-00001"
    prefix = path_parts[1] 
    
    bucket = storage_client.bucket(bucket_name)
    _log_json("INFO", "Searching for Vertex AI batch prediction output files.", bucket=bucket_name, prefix=prefix)
    
    # List all blobs matching the prefix (Vertex AI output directory)
    # Vertex AI Batch Prediction output files are typically named `prediction.results-xxxxx-of-yyyyy`
    # or `predictions_001.jsonl` etc. if sharded.
    blobs = bucket.list_blobs(prefix=prefix)
    
    found_predictions_file = False
    for blob in blobs:
        # Check for common prediction output file patterns
        if "prediction.results" in blob.name and blob.name.endswith(".jsonl"): # More robust check
            found_predictions_file = True
            _log_json("INFO", "Found Vertex AI predictions file.", blob_name=blob.name)
            
            predictions_bytes = _download_gcs_blob_as_bytes(storage_client, bucket_name, blob.name)
            if predictions_bytes:
                try:
                    for line_number, line in enumerate(predictions_bytes.decode('utf-8').splitlines()):
                        if not line.strip(): continue # Skip empty lines
                        try:
                            # Vertex AI batch prediction output format can vary slightly.
                            # Common: {"instance": {original_instance_fields...}, "prediction": {handler_output...}}
                            # Or sometimes the handler output is directly merged if the handler returns a single JSON.
                            # The handler currently returns: {"instance_id": ..., "detected": ..., ...}
                            # Vertex AI wraps this in a "prediction" field if the handler output is a simple dict.
                            # If the handler output *is* the instance_id, then it might be direct.
                            # Let's assume the handler output is under "prediction" key from Vertex.
                            entry = json.loads(line)
                            
                            # Extract instance data (from original input) and prediction data (from model handler)
                            # The `instance` field contains the original JSONL line sent to batch prediction.
                            # The `prediction` field contains the output from your TorchServe handler.
                            instance_data = entry.get("instance", {}) 
                            prediction_data_from_handler = entry.get("prediction", {})

                            # The handler itself includes "instance_id" in its output.
                            # We need to ensure we link correctly. The "instance" field from Vertex
                            # is the most reliable source for the original instance_id.
                            instance_id_from_vertex_instance_field = instance_data.get("instance_id")

                            if instance_id_from_vertex_instance_field:
                                # Merge original instance data with the model's prediction output
                                # Prediction_data_from_handler is what our custom handler returned.
                                prediction_results[instance_id_from_vertex_instance_field] = {
                                    **instance_data, # Contains original gcs_image_uri, region_metadata, etc.
                                    "ai_model_output": prediction_data_from_handler # Contains detected, confidence, etc.
                                }
                            else:
                                _log_json("WARNING", "Skipping batch output line: 'instance_id' missing in 'instance' field.",
                                          line_number=line_number + 1, line_snippet=line[:200])
                        except json.JSONDecodeError as jde:
                            _log_json("ERROR", f"Failed to parse JSONL line from predictions file: {jde}",
                                      line_number=line_number + 1, line_snippet=line[:200], blob_name=blob.name)
                        except Exception as e_line: # Catch other errors processing a line
                            _log_json("ERROR", f"Unexpected error processing predictions line: {e_line}",
                                      line_number=line_number + 1, line_snippet=line[:200], blob_name=blob.name, error_type=type(e_line).__name__)
                except UnicodeDecodeError as ude:
                     _log_json("ERROR", f"Failed to decode predictions file as UTF-8: {ude}", blob_name=blob.name)
            # else: # Already logged by _download_gcs_blob_as_bytes if download fails
            # No break here, process all prediction files in the directory if sharded.
    
    if not found_predictions_file:
        _log_json("WARNING", "No 'prediction.results*.jsonl' files found in batch output directory.", prefix=prefix)

    _log_json("INFO", "Finished parsing Vertex AI batch prediction output.", total_results_parsed=len(prediction_results))
    return prediction_results


# Main Cloud Function entry point
def result_processor_cloud_function(event: Dict, context: Dict):
    """
    Google Cloud Function that processes Vertex AI Batch Prediction results,
    generates maps, and finalizes the fire status report.
    Triggered by a Pub/Sub notification from Vertex AI Batch Job completion.
    """
    _log_json("INFO", "Result Processor Cloud Function triggered.",
               event_id=context.event_id, event_type=context.event_type,
               trigger_resource=context.resource.get("name") if isinstance(context.resource, dict) else str(context.resource),
               timestamp=context.timestamp)

    # --- 0. Configuration & Validation ---
    gcp_project_id = os.environ.get("GCP_PROJECT_ID")
    firms_api_key = os.environ.get("FIRMS_API_KEY") # TODO: Fetch from Secret Manager

    if not all([gcp_project_id, firms_api_key, GCS_BUCKET_NAME]):
        missing_vars = [
            var for var, val in {
                "GCP_PROJECT_ID": gcp_project_id, "FIRMS_API_KEY": firms_api_key,
                "GCS_BUCKET_NAME (from config)": GCS_BUCKET_NAME
            }.items() if not val
        ]
        _log_json("CRITICAL", "Missing one or more required environment variables or configurations.",
                   missing_variables=missing_vars)
        raise ValueError(f"Missing required configurations: {', '.join(missing_vars)}")

    _log_json("INFO", "Environment variables and configurations loaded.", project_id=gcp_project_id)

    # Initialize GCS client once
    storage_client = storage.Client(project=gcp_project_id)

    # Decode the Pub/Sub message data (Vertex AI Batch Job completion notification)
    # https://cloud.google.com/vertex-ai/docs/reference/rpc/google.cloud.aiplatform.v1.JobState
    # https://cloud.google.com/vertex-ai/docs/predictions/get-notifications#pubsub
    if 'data' not in event:
        _log_json("ERROR", "Pub/Sub message 'data' field is missing. Expected Vertex AI job completion notification.")
        raise ValueError("Invalid Pub/Sub message: 'data' field missing.")
    
    try:
        message_data_str = base64.b64decode(event['data']).decode('utf-8')
        message_data = json.loads(message_data_str)
        
        # Extract relevant fields from the notification payload
        # The payload structure for BatchPredictionJob notifications:
        # `payload.batchPredictionJob` contains the job details.
        # `payload.jobState` contains the state.
        # For older notifications, it might be flatter. Let's try to be robust.
        
        payload = message_data.get("payload", message_data) # Handle direct or nested payload
        
        job_resource_name = payload.get("batchPredictionJob", {}).get("name") # Full job resource name
        if not job_resource_name: # Fallback for older/different notification structures
            job_resource_name = payload.get("resourceName", "UnknownJob") # From your previous version

        job_state = payload.get("jobState") # e.g., "JOB_STATE_SUCCEEDED", "JOB_STATE_FAILED"
        if not job_state: # Fallback
             job_state_from_payload = payload.get("batchPredictionJob", {}).get("state") # e.g., "JOB_STATE_SUCCEEDED"
             if job_state_from_payload:
                 job_state = job_state_from_payload
             else: # Try the flatter structure
                 job_state = payload.get("state", "JOB_STATE_UNSPECIFIED")


        # Output info is typically nested
        output_info = payload.get("batchPredictionJob", {}).get("outputInfo", {})
        if not output_info: # Fallback
            output_info = payload.get("metadata", {}).get("outputInfo", {})
            
        output_gcs_uri_prefix = output_info.get("gcsOutputDirectory") # Correct field name
        if not output_gcs_uri_prefix: # Fallback for "outputUriPrefix"
            output_gcs_uri_prefix = output_info.get("outputUriPrefix")

        _log_json("INFO", "Vertex AI Batch Job notification received.",
                   job_name=job_resource_name, job_state=job_state,
                   output_gcs_prefix=output_gcs_uri_prefix,
                   raw_notification_snippet=message_data_str[:500]) # Log snippet for debugging

        if job_state != 'JOB_STATE_SUCCEEDED': # Official state string
            _log_json("WARNING", f"Vertex AI Batch Job '{job_resource_name}' did not succeed. State: {job_state}. Skipping processing.",
                       job_name=job_resource_name, job_state=job_state)
            return # Graceful exit

        if not output_gcs_uri_prefix:
            _log_json("ERROR", "Batch prediction output GCS URI prefix is missing from job notification. Cannot proceed.",
                       job_name=job_resource_name)
            raise ValueError("Missing batch prediction output GCS URI prefix.")

        # Derive acquisition_date_str from instance_id in AI predictions later, if possible,
        # as it's more directly tied to the data processed by the AI.
        # Fallback: try to parse from output_gcs_uri_prefix (less reliable).
        # Example output_gcs_uri_prefix: gs://<bucket>/vertex_ai_batch_outputs/1234567890123456789/
        # The last part is the job ID, not a timestamp.
        # The actual prediction files are inside a subfolder like:
        # gs://<bucket>/vertex_ai_batch_outputs/<job_id>/prediction-<model_id>-<timestamp>/
        # For now, let's assume acquisition_date will come from the AI results' instance_id.
        # A placeholder for now, will be updated per region.
        report_acquisition_date_str = "UnknownDate"


        # --- 1. Fetch AI Prediction Results (from GCS) ---
        _log_json("INFO", "Fetching and parsing Vertex AI batch prediction results.")
        all_ai_predictions_by_instance_id = _parse_vertex_ai_batch_output(storage_client, output_gcs_uri_prefix)
        
        if not all_ai_predictions_by_instance_id:
            _log_json("WARNING", "No AI predictions found or parsed. Finalizing report with limited insights.")
            # Proceed to generate overall status with data unavailable or no fires.
            # We still want to generate a status file even if AI failed.
        else:
            _log_json("INFO", "Successfully parsed AI predictions.", num_predictions=len(all_ai_predictions_by_instance_id))
            # Try to get a common acquisition date from the first valid instance_id
            first_instance_id = next(iter(all_ai_predictions_by_instance_id.keys()), None)
            if first_instance_id and '_' in first_instance_id:
                try:
                    date_part = first_instance_id.split('_')[-1] # Expects format like "regionid_YYYYMMDD"
                    report_acquisition_date_str = datetime.strptime(date_part, '%Y%m%d').strftime('%Y-%m-%d')
                    _log_json("INFO", f"Derived report acquisition date from AI results: {report_acquisition_date_str}")
                except (ValueError, IndexError):
                    _log_json("WARNING", "Could not parse date from first AI instance_id. Report date will be 'UnknownDate'.",
                                       first_instance_id=first_instance_id)


        # --- 2. Re-fetch FIRMS Data (for map visualization and summary) ---
        _log_json("INFO", "Re-fetching FIRMS data for map visualization and summary.")
        firms_retriever = FirmsDataRetriever(
            api_key=firms_api_key,
            base_url=FIRMS_API_BASE_URL,
            sensors=FIRMS_SENSORS
        )
        # This gets FIRMS for the last 24h from now.
        # For map overlays, it's generally fine. For strict historical alignment with imagery,
        # FIRMS API would need to be queried for a specific date range.
        relevant_firms_df = firms_retriever.get_and_filter_firms_data(MONITORED_REGIONS)
        _log_json("INFO", "Relevant FIRMS hotspots retrieved for summary.", count=len(relevant_firms_df))


        # --- 3. Generate Maps and Assemble Final Metadata ---
        _log_json("INFO", "Generating maps and assembling final metadata for monitored areas.")
        map_visualizer = MapVisualizer()
        
        final_monitored_areas_status: List[Dict[str, Any]] = []
        overall_fires_detected_in_report = False # Renamed for clarity

        for region_config in MONITORED_REGIONS: # Iterate through configured regions
            region_id = region_config["id"]
            # Construct the expected instance_id format used by PipelineInitiatorCF
            # This requires knowing the acquisition_date used for that instance.
            # If report_acquisition_date_str is "UnknownDate", this lookup will likely fail.
            current_region_acquisition_date_str = report_acquisition_date_str # Assume same date for all for now
            if current_region_acquisition_date_str == "UnknownDate" and all_ai_predictions_by_instance_id:
                 # Try to find an instance_id specific to this region to get its date
                for inst_id in all_ai_predictions_by_instance_id.keys():
                    if inst_id.startswith(region_id + "_"):
                        try:
                            date_part = inst_id.split('_')[-1]
                            current_region_acquisition_date_str = datetime.strptime(date_part, '%Y%m%d').strftime('%Y-%m-%d')
                            _log_json("INFO", f"Derived acquisition date for region '{region_id}': {current_region_acquisition_date_str}")
                            break
                        except (ValueError, IndexError):
                            pass # Keep as UnknownDate if parsing fails

            instance_id_for_region_lookup = f"{region_id}_{current_region_acquisition_date_str.replace('-', '')}" if current_region_acquisition_date_str != "UnknownDate" else None

            area_status: Dict[str, Any] = {
                "area_id": region_id,
                "area_name": region_config.get("name", "N/A"),
                "status": "DATA_UNAVAILABLE", # Default status
                "acquisition_date": current_region_acquisition_date_str,
                "gcs_map_image_path": None,
                "gcs_source_image_path": None, # Will be populated from AI prediction instance data
                "ai_prediction_summary": {"detected": False, "confidence": 0.0, "details": "N/A"},
                "firms_hotspot_count_in_area": 0,
                "last_updated_utc": datetime.utcnow().isoformat() + "Z",
                "error_details": None
            }

            ai_prediction_for_this_region = None
            if instance_id_for_region_lookup:
                ai_prediction_for_this_region = all_ai_predictions_by_instance_id.get(instance_id_for_region_lookup)

            if ai_prediction_for_this_region:
                # `ai_prediction_for_this_region` contains:
                #   - original instance fields (gcs_image_uri, region_metadata, firms_hotspot_count_in_region)
                #   - "ai_model_output": {detected, confidence, detection_details, error_message from handler}
                
                original_instance_data = ai_prediction_for_this_region # Contains gcs_image_uri etc.
                model_output = original_instance_data.get("ai_model_output", {})

                area_status["gcs_source_image_path"] = original_instance_data.get("gcs_image_uri")
                
                if model_output.get("error_message"):
                    area_status["status"] = "PROCESSING_ERROR"
                    area_status["error_details"] = f"AI Model Handler Error: {model_output['error_message']}"
                    area_status["ai_prediction_summary"]["details"] = "AI processing error"
                    _log_json("WARNING", f"AI model handler reported an error for instance {instance_id_for_region_lookup}.",
                                       error=model_output['error_message'])
                else:
                    area_status["ai_prediction_summary"] = {
                        "detected": model_output.get("detected", False),
                        "confidence": model_output.get("confidence", 0.0),
                        "details": model_output.get("detection_details", "AI processed")
                    }
                    area_status["status"] = "NO_FIRE_DETECTED" # Default if AI processed successfully
                    if area_status["ai_prediction_summary"]["detected"]:
                        area_status["status"] = "FIRE_DETECTED"
                        overall_fires_detected_in_report = True
                        _log_json("INFO", f"AI detected fire in region.", region_id=region_id,
                                           confidence=area_status["ai_prediction_summary"]["confidence"])
                    # else: # No need to log "no fire" explicitly unless for debug
                    #     _log_json("DEBUG", f"AI detected no fire in region.", region_id=region_id)


                # Map Generation
                if area_status["gcs_source_image_path"]:
                    source_img_bucket_name, source_img_blob_name = area_status["gcs_source_image_path"].replace("gs://", "").split("/", 1)
                    source_image_bytes = _download_gcs_blob_as_bytes(storage_client, source_img_bucket_name, source_img_blob_name)
                    
                    if source_image_bytes:
                        # Filter FIRMS hotspots for the current region's bbox for map overlay
                        region_firms_for_map_df = pd.DataFrame() # Empty if no FIRMS data
                        if not relevant_firms_df.empty:
                            min_lon, min_lat, max_lon, max_lat = region_config["bbox"]
                            region_firms_for_map_df = relevant_firms_df[
                                (pd.to_numeric(relevant_firms_df['latitude'], errors='coerce') >= min_lat) &
                                (pd.to_numeric(relevant_firms_df['latitude'], errors='coerce') <= max_lat) &
                                (pd.to_numeric(relevant_firms_df['longitude'], errors='coerce') >= min_lon) &
                                (pd.to_numeric(relevant_firms_df['longitude'], errors='coerce') <= max_lon)
                            ].copy()
                        
                        area_status["firms_hotspot_count_in_area"] = len(region_firms_for_map_df)

                        try:
                            map_image_pil = map_visualizer.generate_fire_map(
                                base_image_bytes=source_image_bytes,
                                image_bbox=region_config["bbox"], # Bbox of the source image/region
                                ai_detections=[area_status["ai_prediction_summary"]], # Pass this region's AI summary
                                firms_hotspots_df=region_firms_for_map_df,
                                acquisition_date_str=current_region_acquisition_date_str
                            )
                            map_image_buffer = io.BytesIO()
                            map_image_pil.save(map_image_buffer, format="PNG")
                            map_image_bytes = map_image_buffer.getvalue()

                            map_filename = f"map_{region_id}_{current_region_acquisition_date_str.replace('-', '')}.png"
                            gcs_map_blob_name = f"{GCS_FINAL_MAPS_DIR}{map_filename}"
                            _upload_gcs_blob_from_bytes(storage_client, GCS_BUCKET_NAME, gcs_map_blob_name, map_image_bytes, content_type="image/png")
                            area_status["gcs_map_image_path"] = f"gs://{GCS_BUCKET_NAME}/{gcs_map_blob_name}"
                            _log_json("INFO", "Map image generated and uploaded.", region_id=region_id, map_path=area_status["gcs_map_image_path"])

                        except Exception as map_e:
                            _log_json("ERROR", f"Error generating or uploading map for region {region_id}: {map_e}", region_id=region_id)
                            if area_status["status"] not in ["PROCESSING_ERROR", "DATA_UNAVAILABLE"]: # Don't overwrite more specific error
                                area_status["status"] = "PROCESSING_ERROR"
                            area_status["error_details"] = (area_status["error_details"] + "; " if area_status["error_details"] else "") + f"Map generation/upload failed: {str(map_e)}"
                    else:
                        _log_json("WARNING", f"Could not download source image for map generation: {area_status['gcs_source_image_path']}.", region_id=region_id)
                        area_status["status"] = "DATA_UNAVAILABLE"
                        area_status["error_details"] = (area_status["error_details"] + "; " if area_status["error_details"] else "") + "Source satellite image for map was not found or unreadable."
                else: # No gcs_source_image_path from AI result
                    _log_json("WARNING", f"No GCS source image URI found in AI result for region {region_id}. Cannot generate map.", region_id=region_id)
                    area_status["status"] = "DATA_UNAVAILABLE"
                    area_status["error_details"] = (area_status["error_details"] + "; " if area_status["error_details"] else "") + "Source image URI missing from AI prediction results, map not generated."
            else: # No AI prediction result for this region_id and derived date
                _log_json("WARNING", f"No AI prediction result found for instance_id '{instance_id_for_region_lookup}'.", region_id=region_id)
                area_status["status"] = "DATA_UNAVAILABLE"
                area_status["error_details"] = f"AI prediction result not found for instance ID '{instance_id_for_region_lookup}'."
            
            final_monitored_areas_status.append(area_status)

        # Determine overall status message for the report
        if overall_fires_detected_in_report:
            overall_status_summary_message = "Fire activity detected by AI in one or more monitored areas."
        elif any(area['status'] == 'FIRE_DETECTED' for area in final_monitored_areas_status): # Fallback if AI didn't run but FIRMS might indicate
            overall_status_summary_message = "Potential fire activity indicated (check FIRMS data or map details)."
        elif all(area['status'] in ['NO_FIRE_DETECTED', 'DATA_UNAVAILABLE'] for area in final_monitored_areas_status):
             overall_status_summary_message = "No significant fire activity detected by AI in processed areas."
        else: # Mix of errors, unavailable, etc.
            overall_status_summary_message = "Wildfire status processed; check individual area details."
        
        # --- 4. Assemble and Store Final wildfire_status_latest.json ---
        final_status_report = {
            "report_generated_utc": datetime.utcnow().isoformat() + "Z",
            "report_for_acquisition_date": report_acquisition_date_str, # Date of the source imagery/data
            "overall_status_summary": overall_status_summary_message,
            "monitored_areas": final_monitored_areas_status,
            "data_sources_info": { # Renamed for clarity
                "firms_last_checked_utc": datetime.utcnow().isoformat() + "Z", # When FIRMS was queried for this report
                "satellite_imagery_acquisition_date": report_acquisition_date_str
            }
        }

        final_status_json_output = json.dumps(final_status_report, indent=2)
        final_status_gcs_blob_name = f"{GCS_METADATA_DIR}{FINAL_STATUS_FILENAME}"
        _upload_gcs_blob_from_bytes(storage_client, GCS_BUCKET_NAME, final_status_gcs_blob_name,
                                    final_status_json_output.encode('utf-8'), content_type="application/json")
        _log_json("INFO", "Final wildfire status report uploaded to GCS.",
                   path=f"gs://{GCS_BUCKET_NAME}/{final_status_gcs_blob_name}")

    except ValueError as ve: # Catch config errors
        _log_json("CRITICAL", f"Configuration or Pub/Sub message parsing error: {str(ve)}", error_type=type(ve).__name__)
        raise
    except Exception as e:
        _log_json("CRITICAL", "An unhandled error occurred during result processing.",
                   error=str(e), error_type=type(e).__name__)
        raise e # Re-raise to signal failure

    _log_json("INFO", "Result Processor Cloud Function execution finished.")


# --- Local Testing Entrypoint ---
if __name__ == "__main__":
    print("--- Running local test for Result Processor Cloud Function ---")

    os.environ["GCP_PROJECT_ID"] = "your-gcp-project-id" # REPLACE
    os.environ["FIRMS_API_KEY"] = "YOUR_FIRMS_API_KEY" # REPLACE
    # GCS_BUCKET_NAME is from common.config

    if GCS_BUCKET_NAME == "fire-app-bucket": # Or your actual one
        print(f"Using configured GCS_BUCKET_NAME: {GCS_BUCKET_NAME} for local test.")
    else:
        print(f"ERROR: GCS_BUCKET_NAME in src/common/config.py ('{GCS_BUCKET_NAME}') seems incorrect.")
        exit(1)

    # For local testing, you need to simulate:
    # 1. A Pub/Sub message (Vertex AI job completion).
    # 2. Vertex AI batch prediction output files in GCS (e.g., prediction.results-xxxxx-of-yyyyy.jsonl).
    # 3. The source satellite images in GCS that are referenced in the batch prediction output.
    # 4. FIRMS API key needs to be valid for FIRMS data fetching.

    # --- Simulate GCS environment for local test ---
    mock_storage_client = storage.Client(project=os.environ["GCP_PROJECT_ID"])
    
    # Example: Create dummy source image and upload
    dummy_region_id = MONITORED_REGIONS[0]["id"] # e.g., "california_central"
    dummy_acq_date = (datetime.utcnow() - timedelta(days=1)).strftime("%Y%m%d")
    dummy_source_image_blob_name = f"raw_satellite_imagery/wildfire_imagery_{dummy_region_id}_{dummy_acq_date}.tif"
    dummy_source_image_gcs_uri = f"gs://{GCS_BUCKET_NAME}/{dummy_source_image_blob_name}"

    if not mock_storage_client.bucket(GCS_BUCKET_NAME).blob(dummy_source_image_blob_name).exists():
        try:
            from PIL import Image as PILImage # Alias to avoid conflict with module name
            dummy_pil_image = PILImage.new('RGB', (200, 200), color = (100, 150, 100))
            img_byte_arr = io.BytesIO()
            dummy_pil_image.save(img_byte_arr, format='TIFF')
            _upload_gcs_blob_from_bytes(mock_storage_client, GCS_BUCKET_NAME, dummy_source_image_blob_name,
                                        img_byte_arr.getvalue(), content_type="image/tiff")
            print(f"Uploaded dummy source image to: {dummy_source_image_gcs_uri}")
        except Exception as upload_err:
            print(f"WARNING: Could not upload dummy source image for local test: {upload_err}")


    # Example: Create dummy Vertex AI prediction output file
    dummy_instance_id = f"{dummy_region_id}_{dummy_acq_date}"
    dummy_predictions_content = json.dumps({
        "instance": {
            "instance_id": dummy_instance_id,
            "gcs_image_uri": dummy_source_image_gcs_uri,
            "region_metadata": MONITORED_REGIONS[0],
            "firms_hotspot_count_in_region": 3
        },
        "prediction": { # This is what your handler returns
            "instance_id": dummy_instance_id, # Handler includes it
            "detected": True,
            "confidence": 0.92,
            "detection_details": "Simulated fire detected by AI (local test)",
            "error_message": None
        }
    }) + "\n" # JSONL format

    # Simulate Vertex AI output path structure
    # gs://<bucket>/<prefix_from_notification>/prediction.results-00000-of-00001
    mock_job_id = "local_test_job_12345"
    mock_vertex_output_dir_prefix = f"gs://{GCS_BUCKET_NAME}/vertex_ai_batch_outputs_test/{mock_job_id}/"
    # The actual file would be inside a sub-folder like prediction-<model_id>-<timestamp>
    # For simplicity, let's put it directly under job_id for local test.
    # Or, more accurately, the notification gives the prefix *up to* the job_id.
    # The files are then inside that.
    mock_predictions_blob_name = f"vertex_ai_batch_outputs_test/{mock_job_id}/prediction.results-00000-of-00001.jsonl"

    try:
        _upload_gcs_blob_from_bytes(mock_storage_client, GCS_BUCKET_NAME, mock_predictions_blob_name,
                                    dummy_predictions_content.encode('utf-8'), content_type="application/jsonl")
        print(f"Uploaded dummy predictions file to: gs://{GCS_BUCKET_NAME}/{mock_predictions_blob_name}")
    except Exception as upload_err:
        print(f"WARNING: Could not upload dummy predictions file for local test: {upload_err}")


    # Mock Pub/Sub event for Vertex AI Batch Job completion
    mock_event_payload = {
        "payload": { # Simulating the richer payload structure
            "batchPredictionJob": {
                "name": f"projects/{os.environ['GCP_PROJECT_ID']}/locations/us-central1/batchPredictionJobs/{mock_job_id}",
                "state": "JOB_STATE_SUCCEEDED", # Critical field
                "outputInfo": {
                    "gcsOutputDirectory": mock_vertex_output_dir_prefix # Path to the directory
                }
            },
            "jobState": "JOB_STATE_SUCCEEDED" # Redundant but sometimes present
        }
    }
    mock_event_data_str = json.dumps(mock_event_payload)
    mock_event_data_b64 = base64.b64encode(mock_event_data_str.encode('utf-8')).decode('utf-8')
    mock_event = {"data": mock_event_data_b64}

    class MockContext: # Simplified context
        def __init__(self, event_id="test-event-local-proc-456", timestamp=datetime.utcnow().isoformat() + "Z"):
            self.event_id = event_id
            self.timestamp = timestamp
            self.event_type = "google.cloud.pubsub.topic.v1.messagePublished"
            self.resource = {"name": "projects/your-gcp-project-id/topics/your-vertex-notifications-topic", "service": "pubsub.googleapis.com"}
    mock_context_obj = MockContext()

    try:
        result_processor_cloud_function(mock_event, mock_context_obj)
        print("--- Local test of Result Processor completed. Check logs and GCS for 'final_outputs'. ---")
    except ValueError as ve:
        print(f"--- Local test failed due to ValueError: {ve} ---")
    except Exception as e:
        print(f"--- Local test failed with an unexpected error: {e} ---")
        import traceback
        traceback.print_exc()
</file>

<file path="src/cloud_functions/result_processor/requirements.txt">
# src/cloud_functions/result_processor/requirements.txt
google-cloud-storage
pandas # For FirmsDataRetriever and for processing AI results (though AI results are now mainly JSON)
Pillow # For MapVisualizer
requests # For FirmsDataRetriever
</file>

<file path="src/map_visualizer/visualizer.py">
# src/map_visualizer/visualizer.py

import logging
import json # For _log_json
import io # For BytesIO
import os # For checking font file existence
from PIL import Image, ImageDraw, ImageFont # Correct Pillow imports
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
import pandas as pd # Added import for type hinting firms_hotspots_df

# --- Logging Setup ---
# logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s') # Can be removed
logger = logging.getLogger(__name__) # Standard

def _log_json(severity: str, message: str, **kwargs):
    """
    Helper to log structured JSON messages to stdout for GCP Cloud Logging.
    """
    log_entry = {
        "severity": severity.upper(),
        "message": message,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "component": "MapVisualizer",
        **kwargs
    }
    print(json.dumps(log_entry))


class MapVisualizer:
    """
    Component 4: Creates a user-friendly map image showing fire detections
    overlaid on a satellite image.
    """

    def __init__(self, default_font_size: int = 16, fir_marker_radius: int = 6, ai_marker_size: int = 25):
        """
        Initializes the MapVisualizer with configurable drawing parameters.

        Args:
            default_font_size (int): Default font size for text overlays.
            fir_marker_radius (int): Radius for FIRMS hotspot markers.
            ai_marker_size (int): Size for AI detection markers.
        """
        _log_json("INFO", "MapVisualizer initialized.")
        self.default_font_size = default_font_size
        self.fir_marker_radius = fir_marker_radius
        self.ai_marker_size = ai_marker_size
        
        # Attempt to load a preferred font, fallback to default
        # Common paths for DejaVuSans-Bold.ttf
        font_paths_to_try = [
            "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", # Common on Linux
            "DejaVuSans-Bold.ttf", # If in current dir or system path
            # Add other common paths if needed, e.g., for macOS or Windows if testing locally there
            # "/Library/Fonts/DejaVuSans-Bold.ttf",
            # "C:/Windows/Fonts/arialbd.ttf" # Example fallback to Arial Bold on Windows
        ]
        self.font = ImageFont.load_default() # Start with default
        for path in font_paths_to_try:
            try:
                if os.path.exists(path):
                    self.font = ImageFont.truetype(path, self.default_font_size)
                    _log_json("INFO", f"Successfully loaded font '{path}' with size {self.default_font_size}.")
                    break # Found a font, no need to try others
            except Exception as e:
                _log_json("WARNING", f"Could not load font at '{path}', trying next. Error: {e}")
        
        if self.font == ImageFont.load_default(): # Check if still default after trying
             _log_json("WARNING", "Using default PIL font. For better text, install a TTF font (e.g., DejaVuSans-Bold.ttf) "
                                "and ensure it's accessible.", tried_paths=font_paths_to_try)


    def _get_pixel_coords(self, img_width: int, img_height: int, image_geospatial_bbox: List[float],
                          point_latitude: float, point_longitude: float) -> Optional[Tuple[int, int]]:
        """
        Converts a geographic coordinate (lat, lon) to pixel coordinates (x, y)
        within an image given its geographic bounding box.

        Args:
            img_width (int): Width of the image in pixels.
            img_height (int): Height of the image in pixels.
            image_geospatial_bbox (List[float]): Bounding box of the image [min_lon, min_lat, max_lon, max_lat].
            point_latitude (float): Latitude of the point to convert.
            point_longitude (float): Longitude of the point to convert.

        Returns:
            Optional[Tuple[int, int]]: Pixel coordinates (x, y), or None if inputs are invalid (e.g., zero range bbox).
        """
        min_lon, min_lat, max_lon, max_lat = image_geospatial_bbox

        # Validate bbox to prevent division by zero
        if max_lon == min_lon or max_lat == min_lat:
            _log_json("ERROR", "Invalid image_geospatial_bbox: longitude or latitude range is zero.",
                      bbox=image_geospatial_bbox)
            return None

        # Calculate normalized position (0 to 1)
        # Ensure point is within or can be mapped relative to the bbox
        norm_lon = (point_longitude - min_lon) / (max_lon - min_lon)
        norm_lat = (point_latitude - min_lat) / (max_lat - min_lat) # For latitude, larger value is "more north"

        # Convert to pixel coordinates. Y-axis is inverted for images (0 at top).
        pixel_x = int(norm_lon * img_width)
        # For Y: if norm_lat is 0 (min_lat), pixel_y should be img_height.
        # If norm_lat is 1 (max_lat), pixel_y should be 0.
        pixel_y = int((1.0 - norm_lat) * img_height)

        # Clamp to image boundaries to ensure markers are drawn within the image,
        # even if the geo-coordinate is slightly outside the provided bbox.
        pixel_x = max(0, min(pixel_x, img_width - 1))
        pixel_y = max(0, min(pixel_y, img_height - 1))

        return pixel_x, pixel_y

    def generate_fire_map(
        self,
        base_image_bytes: bytes,
        image_bbox: List[float], # Geographic BBOX of the base_image_bytes
        ai_detections: List[Dict[str, Any]], # List of AI detection dicts for this image/region
        firms_hotspots_df: Optional[pd.DataFrame] = None,
        acquisition_date_str: str = "N/A" # Date of imagery acquisition
    ) -> Image.Image:
        """
        Generates a composite map image with fire detections and FIRMS hotspots.

        Args:
            base_image_bytes (bytes): The raw bytes of the satellite image (e.g., JPEG, PNG, TIFF).
            image_bbox (List[float]): The geographic bounding box of the base_image
                                      [min_lon, min_lat, max_lon, max_lat].
            ai_detections (List[Dict[str, Any]]): List of AI detection results.
                                                  Expected: {"detected": true/false, "confidence": float, ...}
                                                  (Assumes these detections pertain to this image_bbox)
            firms_hotspots_df (Optional[pd.DataFrame]): DataFrame of FIRMS hotspots
                                                        with 'latitude', 'longitude', 'confidence' columns.
            acquisition_date_str (str): Date of imagery acquisition in 'YYYY-MM-DD' format or "N/A".

        Returns:
            PIL.Image.Image: The generated composite map image.
        
        Raises:
            IOError: If base_image_bytes cannot be opened by Pillow.
            ValueError: If image_bbox is invalid for coordinate conversion.
        """
        _log_json("INFO", "Starting map generation process.",
                           acquisition_date=acquisition_date_str,
                           num_ai_detections_provided=len(ai_detections),
                           num_firms_hotspots_provided=len(firms_hotspots_df) if firms_hotspots_df is not None else 0)

        try:
            base_image = Image.open(io.BytesIO(base_image_bytes)).convert("RGBA") # Use RGBA for transparency options
            draw = ImageDraw.Draw(base_image)
            img_width, img_height = base_image.size
        except IOError as ioe:
            _log_json("ERROR", "Failed to open base image from bytes for map generation.", error=str(ioe))
            raise # Re-raise to be handled by caller

        # --- Overlay FIRMS Hotspots ---
        if firms_hotspots_df is not None and not firms_hotspots_df.empty:
            _log_json("INFO", "Overlaying FIRMS hotspots onto map.", count=len(firms_hotspots_df))
            for _, row in firms_hotspots_df.iterrows():
                try:
                    # Ensure lat/lon are numeric, skip if not (already coerced in retriever, but good check)
                    lat = float(row['latitude'])
                    lon = float(row['longitude'])
                except (ValueError, TypeError):
                    _log_json("WARNING", "Skipping FIRMS hotspot with invalid lat/lon.", data=row.to_dict())
                    continue

                confidence = str(row.get('confidence', 'unknown')).lower()
                pixel_coords = self._get_pixel_coords(img_width, img_height, image_bbox, lat, lon)

                if pixel_coords:
                    px, py = pixel_coords
                    radius = self.fir_marker_radius
                    # Define colors based on confidence
                    if confidence == 'high': firms_color = (255, 0, 0, 200) # Red, slightly transparent
                    elif confidence == 'nominal': firms_color = (255, 165, 0, 200) # Orange
                    else: firms_color = (255, 255, 0, 180) # Yellow (for low or unknown)
                    
                    # Draw a filled circle with an outline
                    draw.ellipse((px - radius, py - radius, px + radius, py + radius),
                                 fill=firms_color, outline=(0, 0, 0, 220)) # Black outline
                else:
                    _log_json("WARNING", "Could not get pixel coordinates for FIRMS hotspot.", lat=lat, lon=lon)
        
        # --- Overlay AI Detections ---
        # For MVP, AI detection is per region. If 'detected' is true, mark the center.
        # A more advanced model might provide bounding boxes or segmentation masks.
        fire_detected_by_ai_in_this_image = False
        ai_confidence_for_this_image = 0.0

        if ai_detections: # ai_detections is a list, usually with one item for the current region/image
            # Process the first (or most relevant) AI detection for this image
            # In ResultProcessorCF, we pass only the relevant AI detection for the current image.
            primary_ai_detection = ai_detections[0]
            if primary_ai_detection.get("detected", False):
                fire_detected_by_ai_in_this_image = True
                ai_confidence_for_this_image = primary_ai_detection.get("confidence", 0.0)

                # Calculate the center of the image_bbox (which is the region bbox)
                center_lon = (image_bbox[0] + image_bbox[2]) / 2
                center_lat = (image_bbox[1] + image_bbox[3]) / 2
                
                pixel_coords_center = self._get_pixel_coords(img_width, img_height, image_bbox, center_lat, center_lon)

                if pixel_coords_center:
                    px_center, py_center = pixel_coords_center
                    marker_sz = self.ai_marker_size
                    ai_marker_color = (0, 255, 255, 220) # Cyan, slightly transparent
                    ai_text = f"AI: Fire ({ai_confidence_for_this_image:.2f})"
                    
                    # Draw a cross marker
                    draw.line((px_center - marker_sz, py_center, px_center + marker_sz, py_center), fill=ai_marker_color, width=3)
                    draw.line((px_center, py_center - marker_sz, px_center, py_center + marker_sz), fill=ai_marker_color, width=3)
                    
                    # Add text next to marker
                    # Calculate text size to position it nicely (Pillow 9.2.0+ for textbbox, older use textsize)
                    try:
                        text_bbox = draw.textbbox((0,0), ai_text, font=self.font)
                        text_width = text_bbox[2] - text_bbox[0]
                        text_height = text_bbox[3] - text_bbox[1] # Not strictly needed for x-offset
                    except AttributeError: # Fallback for older Pillow
                        text_width, text_height = draw.textsize(ai_text, font=self.font)

                    text_pos_x = px_center + marker_sz // 2 + 5
                    text_pos_y = py_center - text_height // 2 # Vertically center text with marker
                    # Ensure text is within image bounds
                    text_pos_x = min(text_pos_x, img_width - text_width - 5)
                    text_pos_y = max(5, min(text_pos_y, img_height - text_height - 5))

                    draw.text((text_pos_x, text_pos_y), ai_text, fill=ai_marker_color, font=self.font)
                    _log_json("INFO", "AI fire detection marker drawn on map.", confidence=ai_confidence_for_this_image)
            else: # AI processed, but no fire detected
                 _log_json("INFO", "AI processed this image: No fire detected.",
                           confidence=primary_ai_detection.get("confidence", 0.0))
        else: # No AI detections provided for this image
            _log_json("INFO", "No AI detection results were provided for this image map.")

        # --- Add Timestamp and Legend ---
        # Text properties
        text_color = (255, 255, 255, 255) # White, opaque
        shadow_color = (0, 0, 0, 200)    # Black, slightly transparent shadow for better readability

        # Acquisition Date Text
        date_text_content = f"Imagery Date: {acquisition_date_str}"
        # Position at bottom-left
        date_text_pos_y = img_height - self.default_font_size - 10 # 10px padding from bottom
        # Shadow first
        draw.text((11, date_text_pos_y + 1), date_text_content, font=self.font, fill=shadow_color)
        # Actual text
        draw.text((10, date_text_pos_y), date_text_content, font=self.font, fill=text_color)

        # Legend (simple version at bottom-right or top-right)
        legend_item_height = self.default_font_size + 4
        legend_y_start = img_height - 10 # Start 10px from bottom
        legend_x_start = img_width - 200 # Start 200px from right (adjust based on expected text length)
        
        # FIRMS High Confidence
        lgd_y = legend_y_start - legend_item_height
        draw.ellipse((legend_x_start, lgd_y - self.fir_marker_radius // 2, 
                      legend_x_start + self.fir_marker_radius, lgd_y + self.fir_marker_radius // 2),
                     fill=(255, 0, 0, 200), outline=(0,0,0))
        draw.text((legend_x_start + self.fir_marker_radius + 5, lgd_y - self.default_font_size // 2 -2),
                  "FIRMS (High)", fill=text_color, font=self.font)

        # FIRMS Nominal Confidence
        lgd_y -= legend_item_height
        draw.ellipse((legend_x_start, lgd_y - self.fir_marker_radius // 2, 
                      legend_x_start + self.fir_marker_radius, lgd_y + self.fir_marker_radius // 2),
                     fill=(255, 165, 0, 200), outline=(0,0,0))
        draw.text((legend_x_start + self.fir_marker_radius + 5, lgd_y - self.default_font_size // 2 -2),
                  "FIRMS (Nominal)", fill=text_color, font=self.font)

        # AI Detection Legend (if fire detected by AI)
        if fire_detected_by_ai_in_this_image:
            lgd_y -= legend_item_height
            # Draw a small cross for AI legend
            ai_lgd_marker_sz = self.ai_marker_size // 3
            ai_lgd_marker_center_x = legend_x_start + ai_lgd_marker_sz // 2
            ai_lgd_marker_center_y = lgd_y # Align with text baseline

            draw.line((ai_lgd_marker_center_x - ai_lgd_marker_sz, ai_lgd_marker_center_y,
                       ai_lgd_marker_center_x + ai_lgd_marker_sz, ai_lgd_marker_center_y),
                      fill=(0, 255, 255, 220), width=2)
            draw.line((ai_lgd_marker_center_x, ai_lgd_marker_center_y - ai_lgd_marker_sz,
                       ai_lgd_marker_center_x, ai_lgd_marker_center_y + ai_lgd_marker_sz),
                      fill=(0, 255, 255, 220), width=2)
            draw.text((legend_x_start + self.fir_marker_radius + 5, lgd_y - self.default_font_size // 2 -2), # Re-align text X with others
                      "AI Detection", fill=(0, 255, 255, 220), font=self.font)


        _log_json("INFO", "Map generation completed successfully.")
        return base_image

    # Note: The local testing block from the original was good.
    # It can be adapted if MapVisualizer is run standalone.
    # For brevity here, I'll omit it but assume it's similar to the original.

# --- Example Usage (for local testing) ---
if __name__ == "__main__":
    _log_json("INFO", "Running local test for MapVisualizer.")

    # Create a dummy base image (e.g., from Pillow)
    dummy_image_width, dummy_image_height = 800, 600
    dummy_base_pil_image = Image.new('RGB', (dummy_image_width, dummy_image_height), color = (73, 109, 137)) # Blue-ish
    buffer = io.BytesIO()
    dummy_base_pil_image.save(buffer, format="PNG")
    dummy_image_bytes = buffer.getvalue()

    # Define a dummy bounding box for the image
    dummy_image_bbox = [-122.0, 36.0, -118.0, 38.0] # california_central from config

    # Dummy FIRMS hotspots
    dummy_firms_data = pd.DataFrame({
        'latitude': [37.5, 37.2, 36.8, 36.1], # Added one more
        'longitude': [-121.5, -119.5, -118.5, -121.8],
        'confidence': ['high', 'nominal', 'low', 'high'], # Added 'low'
    })
    
    # Dummy AI detection results
    ai_fire_detected = [{"detected": True, "confidence": 0.91, "details": "Fire confirmed by AI"}]
    ai_no_fire_detected = [{"detected": False, "confidence": 0.12, "details": "No fire confirmed by AI"}]

    visualizer = MapVisualizer(default_font_size=18, fir_marker_radius=7, ai_marker_size=30)

    try:
        _log_json("INFO", "Generating map with AI fire detection and FIRMS hotspots...")
        map_with_fire = visualizer.generate_fire_map(
            base_image_bytes=dummy_image_bytes,
            image_bbox=dummy_image_bbox,
            ai_detections=ai_fire_detected,
            firms_hotspots_df=dummy_firms_data,
            acquisition_date_str="2024-01-15"
        )
        map_with_fire.save("test_map_fire_detected_refined.png")
        _log_json("INFO", "Map with fire saved as test_map_fire_detected_refined.png")

        _log_json("INFO", "Generating map with NO AI fire detection and FIRMS hotspots...")
        map_no_fire = visualizer.generate_fire_map(
            base_image_bytes=dummy_image_bytes,
            image_bbox=dummy_image_bbox,
            ai_detections=ai_no_fire_detected,
            firms_hotspots_df=dummy_firms_data, # Still show FIRMS
            acquisition_date_str="2024-01-16"
        )
        map_no_fire.save("test_map_no_fire_detected_refined.png")
        _log_json("INFO", "Map with no AI fire saved as test_map_no_fire_detected_refined.png")

        _log_json("INFO", "Generating map with NO FIRMS data (AI fire detected)...")
        map_no_firms = visualizer.generate_fire_map(
            base_image_bytes=dummy_image_bytes,
            image_bbox=dummy_image_bbox,
            ai_detections=ai_fire_detected,
            firms_hotspots_df=None, # No FIRMS data
            acquisition_date_str="2024-01-17"
        )
        map_no_firms.save("test_map_no_firms_refined.png")
        _log_json("INFO", "Map with no FIRMS saved as test_map_no_firms_refined.png")

    except Exception as e:
        _log_json("CRITICAL", f"Local map visualizer test failed: {e}", error_type=type(e).__name__)
        import traceback
        traceback.print_exc()
</file>

<file path="src/ml_model/handler.py">
# src/ml_model/handler.py

import os
import io
import logging
import json
import torch
import torch.nn.functional as F
from PIL import Image
from google.cloud import storage
from ts.torch_handler.base_handler import BaseHandler
from datetime import datetime

# Import the model definition and transforms
from fire_detection_model import DummyFireDetectionModel, MODEL_INPUT_TRANSFORMS

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def _log_json(severity: str, message: str, **kwargs):
    log_entry = {
        "severity": severity.upper(),
        "message": message,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "component": "TorchServeHandler",
        **kwargs
    }
    print(json.dumps(log_entry))


class FireDetectionHandler(BaseHandler):
    def __init__(self):
        super().__init__()
        self.initialized = False
        self.storage_client = None
        _log_json("INFO", "FireDetectionHandler instance created.")

    def initialize(self, context):
        properties = context.system_properties
        model_dir = properties.get("model_dir")
        self.device = torch.device("cuda" if torch.cuda.is_available() and properties.get("gpu_id") is not None else "cpu")

        _log_json("INFO", f"Initializing handler on device: {self.device}")

        self.model = DummyFireDetectionModel()
        model_pt_path = os.path.join(model_dir, "model.pth")
        if not os.path.exists(model_pt_path):
            _log_json("ERROR", f"Model state_dict not found at: {model_pt_path}")
            raise FileNotFoundError(f"Missing model state_dict: {model_pt_path}")

        try:
            self.model.load_state_dict(torch.load(model_pt_path, map_location=self.device))
            self.model.to(self.device)
            self.model.eval()
            _log_json("INFO", "Model loaded successfully.", model_path=model_pt_path, device=str(self.device))
        except Exception as e:
            _log_json("CRITICAL", f"Failed to load model state_dict: {e}", model_path=model_pt_path)
            raise RuntimeError(f"Failed to load model: {e}")

        try:
            self.storage_client = storage.Client()
            _log_json("INFO", "Google Cloud Storage client initialized.")
        except Exception as e:
            _log_json("CRITICAL", f"Failed to initialize GCS client: {e}")
            raise RuntimeError(f"Failed to initialize GCS client: {e}")

        self.initialized = True
        _log_json("INFO", "FireDetectionHandler initialization complete.")

    def preprocess(self, data: list) -> tuple[list[torch.Tensor | None], list[dict]]:
        _log_json("INFO", f"Starting preprocessing for {len(data)} instances.")
        
        processed_tensors = []
        instance_metadata_list = [] 

        for i, row in enumerate(data):
            original_input_data = {} 
            current_instance_id_for_log = f"instance_{i}_parsing_failed" # Default if parsing row fails

            try:
                if isinstance(row, dict) and 'data' in row and isinstance(row['data'], bytes):
                    input_data = json.loads(row['data'].decode('utf-8'))
                    original_input_data = input_data 
                elif isinstance(row, dict):
                    input_data = row
                    original_input_data = row 
                else:
                    err_msg = f"Unexpected input data format for instance {i}."
                    _log_json("ERROR", err_msg, input_type=type(row), input_snippet=str(row)[:100])
                    processed_tensors.append(None) 
                    instance_metadata_list.append({"instance_id": current_instance_id_for_log, "preprocess_error": err_msg})
                    continue
                
                # Successfully parsed row, now store metadata and get instance_id
                instance_metadata_list.append(original_input_data)
                current_instance_id_for_log = original_input_data.get("instance_id", f"instance_{i}")

                gcs_image_uri = input_data.get("gcs_image_uri")
                if not gcs_image_uri:
                    err_msg = f"Missing 'gcs_image_uri' for instance {current_instance_id_for_log}."
                    _log_json("ERROR", err_msg, instance_id=current_instance_id_for_log)
                    processed_tensors.append(None) 
                    instance_metadata_list[-1]["preprocess_error"] = err_msg # Add error to last appended metadata
                    continue

                _log_json("INFO", f"Downloading image from GCS.", instance_id=current_instance_id_for_log, uri=gcs_image_uri)
                parts = gcs_image_uri.replace("gs://", "").split("/", 1)
                if len(parts) < 2:
                    err_msg = f"Invalid GCS URI format: {gcs_image_uri}."
                    _log_json("ERROR", err_msg, instance_id=current_instance_id_for_log)
                    processed_tensors.append(None)
                    instance_metadata_list[-1]["preprocess_error"] = err_msg
                    continue
                
                bucket_name, blob_name = parts[0], parts[1]
                bucket = self.storage_client.bucket(bucket_name)
                blob = bucket.blob(blob_name)

                if not blob.exists():
                    err_msg = f"GCS image blob does not exist: {gcs_image_uri}."
                    _log_json("ERROR", err_msg, instance_id=current_instance_id_for_log)
                    processed_tensors.append(None)
                    instance_metadata_list[-1]["preprocess_error"] = err_msg
                    continue

                image_bytes = blob.download_as_bytes()
                image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
                transformed_image = MODEL_INPUT_TRANSFORMS(image)
                processed_tensors.append(transformed_image)
                _log_json("INFO", "Image preprocessed successfully.", instance_id=current_instance_id_for_log, uri=gcs_image_uri)

            except Exception as e:
                # This catches errors during parsing row, GCS ops, or image transformation
                err_msg = f"Error preprocessing instance {current_instance_id_for_log}: {str(e)}"
                _log_json("ERROR", err_msg, original_input_snippet=str(row)[:200], error_type=type(e).__name__)
                processed_tensors.append(None)
                # If metadata wasn't appended due to early failure (e.g. row parsing), append it now with error
                if len(instance_metadata_list) == len(processed_tensors) -1: # Check if metadata for this item is missing
                    instance_metadata_list.append({"instance_id": current_instance_id_for_log, "preprocess_error": err_msg})
                else: # Metadata was already appended, just add/update the error
                    instance_metadata_list[-1]["preprocess_error"] = err_msg
        
        # Ensure lists are of same length before returning (should be by construction)
        if len(processed_tensors) != len(instance_metadata_list):
            _log_json("CRITICAL", "Mismatch in lengths of processed_tensors and instance_metadata_list.",
                      len_tensors=len(processed_tensors), len_metadata=len(instance_metadata_list))
            # Fallback: Pad metadata if necessary, though this indicates a logic error above.
            while len(instance_metadata_list) < len(processed_tensors):
                instance_metadata_list.append({"instance_id": "fallback_meta_error", "preprocess_error": "Metadata list padding"})


        return processed_tensors, instance_metadata_list

    def inference(self, handler_input: tuple[list[torch.Tensor | None], list[dict]]) -> tuple[list[torch.Tensor | None], list[dict]]:
        processed_tensors, instance_metadata_list = handler_input
        _log_json("INFO", f"Starting inference for {len(processed_tensors)} total instances.")

        valid_tensors = []
        valid_indices = []
        for i, tensor_data in enumerate(processed_tensors):
            if tensor_data is not None:
                valid_tensors.append(tensor_data)
                valid_indices.append(i)

        inference_results_for_valid_inputs = []
        if valid_tensors:
            batch_to_infer = torch.stack(valid_tensors).to(self.device)
            _log_json("INFO", f"Performing inference on a batch of {batch_to_infer.shape[0]} valid instances.")
            with torch.no_grad():
                inference_results_for_valid_inputs = self.model(batch_to_infer)
            _log_json("INFO", "Inference complete for valid instances.")
        else:
            _log_json("INFO", "No valid instances to perform inference on.")

        all_inference_outputs = [None] * len(processed_tensors)
        for i, result_tensor in enumerate(inference_results_for_valid_inputs):
            original_index = valid_indices[i]
            all_inference_outputs[original_index] = result_tensor
        
        return all_inference_outputs, instance_metadata_list

    def postprocess(self, handler_output: tuple[list[torch.Tensor | None], list[dict]]) -> list[dict[str, any]]:
        all_inference_outputs, instance_metadata_list = handler_output
        _log_json("INFO", f"Starting postprocessing for {len(all_inference_outputs)} total instances.")
        
        results = []

        for i in range(len(all_inference_outputs)):
            inference_output_tensor = all_inference_outputs[i]
            # Ensure metadata list is not shorter than output list (should not happen with current logic)
            if i >= len(instance_metadata_list):
                _log_json("ERROR", "Metadata list shorter than inference output list during postprocessing.",
                          index=i, len_metadata=len(instance_metadata_list), len_outputs=len(all_inference_outputs))
                # Create a fallback metadata to avoid crashing
                metadata = {"instance_id": f"missing_metadata_instance_{i}", 
                            "error": "Metadata missing for this instance in postprocess"}
            else:
                metadata = instance_metadata_list[i]
            
            instance_id = metadata.get("instance_id", f"processed_instance_{i}") 

            if inference_output_tensor is None:
                error_message = metadata.get("preprocess_error", "Processing failed (reason unknown or occurred during inference step for valid preprocess)")
                
                results.append({
                    "instance_id": instance_id,
                    "detected": False, 
                    "confidence": 0.0,
                    "detection_details": "Error during processing.",
                    "error_message": error_message 
                })
                _log_json("WARNING", "Instance failed processing.", instance_id=instance_id, error=error_message)
            else:
                probabilities = F.softmax(inference_output_tensor.unsqueeze(0), dim=1) 
                predicted_class = torch.argmax(probabilities, dim=1).item()
                
                is_detected = (predicted_class == 1)
                confidence = probabilities[0][predicted_class].item()

                results.append({
                    "instance_id": instance_id,
                    "detected": bool(is_detected),
                    "confidence": float(confidence),
                    "detection_details": "Fire detected by AI model" if is_detected else "No fire detected by AI model",
                    "error_message": None
                })
        
        _log_json("INFO", "Postprocessing complete.", total_results_generated=len(results))
        return results

# --- Example Usage (for local testing of handler methods, not full TorchServe) ---
if __name__ == '__main__':
    from src.common.config import GCS_BUCKET_NAME 

    _log_json("INFO", "Running local handler test (partial - requires GCS setup and model file).")

    class MockContext:
        def __init__(self, model_dir, gpu_id=None):
            self.system_properties = {"model_dir": model_dir, "gpu_id": gpu_id}
            self.manifest = {"model": {"modelName": "model"}}

    if not os.path.exists("model_artifacts"):
        os.makedirs("model_artifacts")
    
    dummy_model_for_test = DummyFireDetectionModel()
    torch.save(dummy_model_for_test.state_dict(), os.path.join("model_artifacts", "model.pth"))
    _log_json("INFO", "Dummy model file 'model.pth' created for local handler test.")

    mock_context = MockContext("model_artifacts")

    handler = FireDetectionHandler()
    try:
        handler.initialize(mock_context)

        valid_gcs_uri = f"gs://{GCS_BUCKET_NAME}/raw_satellite_imagery/test_image_valid.png" 
        invalid_gcs_uri = f"gs://{GCS_BUCKET_NAME}/raw_satellite_imagery/non_existent_image.png"
        malformed_gcs_uri = "gs:/malformed/uri"
        missing_uri_input = {"instance_id": "test_instance_4_missing_uri", "region_metadata": {"id": "test_region_4"}}
        
        dummy_local_image_path = "dummy_handler_test_image.png"
        if not os.path.exists(dummy_local_image_path):
            Image.new('RGB', (224, 224), color = 'green').save(dummy_local_image_path)
            _log_json("INFO", f"Created dummy local image for GCS upload test: {dummy_local_image_path}")
        
        try:
            if GCS_BUCKET_NAME != "fire-app-bucket":
                 _log_json("WARNING", f"GCS_BUCKET_NAME '{GCS_BUCKET_NAME}' might not be intended for this test. Skipping GCS upload.")
            else:
                storage_client_test = storage.Client()
                bucket_test = storage_client_test.bucket(GCS_BUCKET_NAME)
                blob_test = bucket_test.blob(valid_gcs_uri.replace(f"gs://{GCS_BUCKET_NAME}/", ""))
                if not blob_test.exists():
                    blob_test.upload_from_filename(dummy_local_image_path)
                    _log_json("INFO", f"Uploaded {dummy_local_image_path} to {valid_gcs_uri} for testing.")
                else:
                    _log_json("INFO", f"Test image {valid_gcs_uri} already exists in GCS.")
        except Exception as e:
            _log_json("WARNING", f"Could not ensure test image exists at {valid_gcs_uri} due to: {e}. GCS test case might fail.")

        input_batch_for_preprocess = [
            {"instance_id": "test_instance_1_valid", "gcs_image_uri": valid_gcs_uri, "region_metadata": {"id": "test_region_1"}},
            {"instance_id": "test_instance_2_invalid_uri", "gcs_image_uri": invalid_gcs_uri, "region_metadata": {"id": "test_region_2"}},
            {"instance_id": "test_instance_3_malformed_uri", "gcs_image_uri": malformed_gcs_uri, "region_metadata": {"id": "test_region_3"}},
            missing_uri_input,
            {"instance_id": "test_instance_5_another_valid", "gcs_image_uri": valid_gcs_uri, "region_metadata": {"id": "test_region_5"}} 
        ]
        
        _log_json("INFO", "--- Testing Preprocess ---")
        processed_tensors, metadata_list = handler.preprocess(input_batch_for_preprocess)
        
        for i, tensor in enumerate(processed_tensors):
            meta = metadata_list[i] if i < len(metadata_list) else {"instance_id": "error_meta_missing", "preprocess_error": "Metadata missing"}
            status = "Success" if tensor is not None else f"Failure ({meta.get('preprocess_error', 'Unknown preprocess error')})"
            _log_json("INFO", f"Preprocess instance {meta.get('instance_id')}: {status}")

        _log_json("INFO", "--- Testing Inference ---")
        inference_outputs, final_metadata_list = handler.inference((processed_tensors, metadata_list))

        _log_json("INFO", "--- Testing Postprocess ---")
        final_results = handler.postprocess((inference_outputs, final_metadata_list))
        
        _log_json("INFO", "Final batch results (should include errors for failed instances):")
        for res_idx, res_item in enumerate(final_results): # Changed variable name from res to res_item
            print(f"Result for instance {res_idx}: {json.dumps(res_item, indent=2)}") # Added index for clarity

    except Exception as e:
        _log_json("CRITICAL", f"Unhandled error during handler local test: {e}", error_details=str(e))
        import traceback
        traceback.print_exc()
</file>

<file path="src/ml_model/register_vertex_model.py">
# register_vertex_model.py
from google.cloud import aiplatform
import os

PROJECT_ID = "haryo-kebakaran"
REGION = "asia-southeast2"
BUCKET_NAME = "fire-app-bucket"

aiplatform.init(project=PROJECT_ID, location=REGION)

serving_container_image_uri = f"{REGION}-docker.pkg.dev/{PROJECT_ID}/wildfire-detector-ts/wildfire-detector-ts:latest"
artifact_uri = f"gs://{BUCKET_NAME}/models/dummy_fire_detection/"

try:
    print(f"Attempting to upload model '{PROJECT_ID}' in '{REGION}'...")
    model = aiplatform.Model.upload(
        display_name="dummy_wildfire_detector_v1",
        artifact_uri=artifact_uri,
        serving_container_image_uri=serving_container_image_uri,
        serving_container_predict_route="/predictions/model",
        serving_container_health_route="/ping"
    )
    model.wait()
    print(f"Model uploaded successfully. Resource name: {model.resource_name}")
    # IMPORTANT: Copy the NEW MODEL ID from the end of the 'resource_name' output.
except Exception as e:
    print(f"Error uploading model: {e}")
    print("This might happen if the model with this display_name already exists.")
    print("If it exists, check Vertex AI -> Models to get its ID, or increment version.")
</file>

<file path="src/ml_model/repomix-output.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
Dockerfile
fire_detection_model.py
handler.py
register_vertex_model.py
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="Dockerfile">
FROM pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime

LABEL maintainer="Wildfire Detection Team <dev@example.com>"
LABEL version="0.1.0"
LABEL description="TorchServe container for Wildfire Detection AI Model"

ENV PATH="/opt/conda/bin:${PATH}"
ENV HOME="/home/model-server"
ENV PYTHONUNBUFFERED=TRUE
ENV TEMP_DIR_PATH="/tmp"

WORKDIR ${HOME}

RUN apt-get update && \
    apt-get install -y --no-install-recommends openjdk-11-jdk-headless curl wget && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir --upgrade pip

COPY src/ml_model/requirements.txt .

RUN pip install --no-cache-dir --default-timeout=120 --retries=3 \
    "torchserve==0.7.0" \
    "torch-model-archiver==0.7.0" \
    -r requirements.txt

COPY src/ml_model/fire_detection_model.py .
COPY src/ml_model/handler.py .

COPY src/ml_model/model.mar ./model_store/model.mar

EXPOSE 8080
EXPOSE 8081
EXPOSE 8082

HEALTHCHECK --interval=30s --timeout=10s --start-period=300s --retries=3 \
  CMD curl -f http://localhost:8080/ping || exit 1

ENTRYPOINT ["torchserve"]

CMD ["--start", "--model-store", "/home/model-server/model_store", "--models", "model=model.mar"]
</file>

<file path="fire_detection_model.py">
# src/ml_model/fire_detection_model.py

import torch
import torch.nn as nn
from torchvision.transforms import Compose, Resize, ToTensor, Normalize

class DummyFireDetectionModel(nn.Module):
    """
    A dummy PyTorch model for wildfire detection.
    For MVP, it simply acts as a placeholder and always returns a predefined "detected" state.
    In a real scenario, this would be a trained Convolutional Neural Network (CNN).
    """
    def __init__(self):
        super(DummyFireDetectionModel, self).__init__()
        # For a real model, you'd define your layers here, e.g.:
        # self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        # self.relu = nn.ReLU()
        # self.pool = nn.MaxPool2d(2)
        # self.fc = nn.Linear(..., 2) # e.g., for binary classification (fire/no-fire)

        # We'll use a placeholder parameter so TorchServe finds something to load
        self.dummy_param = nn.Parameter(torch.randn(1))

    def forward(self, x):
        """
        Forward pass for the dummy model.
        In a real model, x would be an image tensor, and you'd pass it through your layers.
        For this dummy, we ignore the input and return a fixed output.
        """
        # For MVP, simulate a fixed output: assume fire detected with 85% confidence
        # Real model would output logits or probabilities, e.g., torch.tensor([[0.15, 0.85]])
        return torch.tensor([[0.15, 0.85]]) # [no_fire_prob, fire_prob]

# Define a transformation pipeline that mimics what a real model would need for input.
# This will be used in the handler's preprocess method.
# Assuming input images are 224x224 RGB for a typical CNN.
MODEL_INPUT_TRANSFORMS = Compose([
    Resize((224, 224)),       # Resize to expected input size
    ToTensor(),               # Convert PIL Image to PyTorch Tensor (HWC to CHW, 0-255 to 0-1)
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization (common)
])

if __name__ == '__main__':
    # Simple test of the dummy model
    model = DummyFireDetectionModel()
    dummy_input = torch.randn(1, 3, 224, 224) # Batch size 1, 3 channels, 224x224 image
    output = model(dummy_input)
    print("Dummy Model Output (logits/probabilities):", output)
    # Convert logits/probabilities to predicted class (0 for no fire, 1 for fire)
    predicted_class = torch.argmax(output, dim=1).item()
    confidence = output.softmax(dim=1)[0][predicted_class].item()
    print(f"Predicted Class: {'Fire' if predicted_class == 1 else 'No Fire'} (Confidence: {confidence:.2f})")

    # Save a dummy state_dict for packaging
    dummy_model_path = "dummy_fire_detection_model.pth"
    torch.save(model.state_dict(), dummy_model_path)
    print(f"Dummy model state_dict saved to {dummy_model_path}")
</file>

<file path="handler.py">
# src/ml_model/handler.py

import os
import io
import logging
import json
import torch
import torch.nn.functional as F
from PIL import Image
from google.cloud import storage
from ts.torch_handler.base_handler import BaseHandler
from datetime import datetime

# Import the model definition and transforms
from fire_detection_model import DummyFireDetectionModel, MODEL_INPUT_TRANSFORMS

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def _log_json(severity: str, message: str, **kwargs):
    log_entry = {
        "severity": severity.upper(),
        "message": message,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "component": "TorchServeHandler",
        **kwargs
    }
    print(json.dumps(log_entry))


class FireDetectionHandler(BaseHandler):
    def __init__(self):
        super().__init__()
        self.initialized = False
        self.storage_client = None
        _log_json("INFO", "FireDetectionHandler instance created.")

    def initialize(self, context):
        properties = context.system_properties
        model_dir = properties.get("model_dir")
        self.device = torch.device("cuda" if torch.cuda.is_available() and properties.get("gpu_id") is not None else "cpu")

        _log_json("INFO", f"Initializing handler on device: {self.device}")

        self.model = DummyFireDetectionModel()
        model_pt_path = os.path.join(model_dir, "model.pth")
        if not os.path.exists(model_pt_path):
            _log_json("ERROR", f"Model state_dict not found at: {model_pt_path}")
            raise FileNotFoundError(f"Missing model state_dict: {model_pt_path}")

        try:
            self.model.load_state_dict(torch.load(model_pt_path, map_location=self.device))
            self.model.to(self.device)
            self.model.eval()
            _log_json("INFO", "Model loaded successfully.", model_path=model_pt_path, device=str(self.device))
        except Exception as e:
            _log_json("CRITICAL", f"Failed to load model state_dict: {e}", model_path=model_pt_path)
            raise RuntimeError(f"Failed to load model: {e}")

        try:
            self.storage_client = storage.Client()
            _log_json("INFO", "Google Cloud Storage client initialized.")
        except Exception as e:
            _log_json("CRITICAL", f"Failed to initialize GCS client: {e}")
            raise RuntimeError(f"Failed to initialize GCS client: {e}")

        self.initialized = True
        _log_json("INFO", "FireDetectionHandler initialization complete.")

    def preprocess(self, data: list) -> tuple[list[torch.Tensor | None], list[dict]]:
        _log_json("INFO", f"Starting preprocessing for {len(data)} instances.")
        
        processed_tensors = []
        instance_metadata_list = [] 

        for i, row in enumerate(data):
            original_input_data = {} 
            current_instance_id_for_log = f"instance_{i}_parsing_failed" # Default if parsing row fails

            try:
                if isinstance(row, dict) and 'data' in row and isinstance(row['data'], bytes):
                    input_data = json.loads(row['data'].decode('utf-8'))
                    original_input_data = input_data 
                elif isinstance(row, dict):
                    input_data = row
                    original_input_data = row 
                else:
                    err_msg = f"Unexpected input data format for instance {i}."
                    _log_json("ERROR", err_msg, input_type=type(row), input_snippet=str(row)[:100])
                    processed_tensors.append(None) 
                    instance_metadata_list.append({"instance_id": current_instance_id_for_log, "preprocess_error": err_msg})
                    continue
                
                # Successfully parsed row, now store metadata and get instance_id
                instance_metadata_list.append(original_input_data)
                current_instance_id_for_log = original_input_data.get("instance_id", f"instance_{i}")

                gcs_image_uri = input_data.get("gcs_image_uri")
                if not gcs_image_uri:
                    err_msg = f"Missing 'gcs_image_uri' for instance {current_instance_id_for_log}."
                    _log_json("ERROR", err_msg, instance_id=current_instance_id_for_log)
                    processed_tensors.append(None) 
                    instance_metadata_list[-1]["preprocess_error"] = err_msg # Add error to last appended metadata
                    continue

                _log_json("INFO", f"Downloading image from GCS.", instance_id=current_instance_id_for_log, uri=gcs_image_uri)
                parts = gcs_image_uri.replace("gs://", "").split("/", 1)
                if len(parts) < 2:
                    err_msg = f"Invalid GCS URI format: {gcs_image_uri}."
                    _log_json("ERROR", err_msg, instance_id=current_instance_id_for_log)
                    processed_tensors.append(None)
                    instance_metadata_list[-1]["preprocess_error"] = err_msg
                    continue
                
                bucket_name, blob_name = parts[0], parts[1]
                bucket = self.storage_client.bucket(bucket_name)
                blob = bucket.blob(blob_name)

                if not blob.exists():
                    err_msg = f"GCS image blob does not exist: {gcs_image_uri}."
                    _log_json("ERROR", err_msg, instance_id=current_instance_id_for_log)
                    processed_tensors.append(None)
                    instance_metadata_list[-1]["preprocess_error"] = err_msg
                    continue

                image_bytes = blob.download_as_bytes()
                image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
                transformed_image = MODEL_INPUT_TRANSFORMS(image)
                processed_tensors.append(transformed_image)
                _log_json("INFO", "Image preprocessed successfully.", instance_id=current_instance_id_for_log, uri=gcs_image_uri)

            except Exception as e:
                # This catches errors during parsing row, GCS ops, or image transformation
                err_msg = f"Error preprocessing instance {current_instance_id_for_log}: {str(e)}"
                _log_json("ERROR", err_msg, original_input_snippet=str(row)[:200], error_type=type(e).__name__)
                processed_tensors.append(None)
                # If metadata wasn't appended due to early failure (e.g. row parsing), append it now with error
                if len(instance_metadata_list) == len(processed_tensors) -1: # Check if metadata for this item is missing
                    instance_metadata_list.append({"instance_id": current_instance_id_for_log, "preprocess_error": err_msg})
                else: # Metadata was already appended, just add/update the error
                    instance_metadata_list[-1]["preprocess_error"] = err_msg
        
        # Ensure lists are of same length before returning (should be by construction)
        if len(processed_tensors) != len(instance_metadata_list):
            _log_json("CRITICAL", "Mismatch in lengths of processed_tensors and instance_metadata_list.",
                      len_tensors=len(processed_tensors), len_metadata=len(instance_metadata_list))
            # Fallback: Pad metadata if necessary, though this indicates a logic error above.
            while len(instance_metadata_list) < len(processed_tensors):
                instance_metadata_list.append({"instance_id": "fallback_meta_error", "preprocess_error": "Metadata list padding"})


        return processed_tensors, instance_metadata_list

    def inference(self, handler_input: tuple[list[torch.Tensor | None], list[dict]]) -> tuple[list[torch.Tensor | None], list[dict]]:
        processed_tensors, instance_metadata_list = handler_input
        _log_json("INFO", f"Starting inference for {len(processed_tensors)} total instances.")

        valid_tensors = []
        valid_indices = []
        for i, tensor_data in enumerate(processed_tensors):
            if tensor_data is not None:
                valid_tensors.append(tensor_data)
                valid_indices.append(i)

        inference_results_for_valid_inputs = []
        if valid_tensors:
            batch_to_infer = torch.stack(valid_tensors).to(self.device)
            _log_json("INFO", f"Performing inference on a batch of {batch_to_infer.shape[0]} valid instances.")
            with torch.no_grad():
                inference_results_for_valid_inputs = self.model(batch_to_infer)
            _log_json("INFO", "Inference complete for valid instances.")
        else:
            _log_json("INFO", "No valid instances to perform inference on.")

        all_inference_outputs = [None] * len(processed_tensors)
        for i, result_tensor in enumerate(inference_results_for_valid_inputs):
            original_index = valid_indices[i]
            all_inference_outputs[original_index] = result_tensor
        
        return all_inference_outputs, instance_metadata_list

    def postprocess(self, handler_output: tuple[list[torch.Tensor | None], list[dict]]) -> list[dict[str, any]]:
        all_inference_outputs, instance_metadata_list = handler_output
        _log_json("INFO", f"Starting postprocessing for {len(all_inference_outputs)} total instances.")
        
        results = []

        for i in range(len(all_inference_outputs)):
            inference_output_tensor = all_inference_outputs[i]
            # Ensure metadata list is not shorter than output list (should not happen with current logic)
            if i >= len(instance_metadata_list):
                _log_json("ERROR", "Metadata list shorter than inference output list during postprocessing.",
                          index=i, len_metadata=len(instance_metadata_list), len_outputs=len(all_inference_outputs))
                # Create a fallback metadata to avoid crashing
                metadata = {"instance_id": f"missing_metadata_instance_{i}", 
                            "error": "Metadata missing for this instance in postprocess"}
            else:
                metadata = instance_metadata_list[i]
            
            instance_id = metadata.get("instance_id", f"processed_instance_{i}") 

            if inference_output_tensor is None:
                error_message = metadata.get("preprocess_error", "Processing failed (reason unknown or occurred during inference step for valid preprocess)")
                
                results.append({
                    "instance_id": instance_id,
                    "detected": False, 
                    "confidence": 0.0,
                    "detection_details": "Error during processing.",
                    "error_message": error_message 
                })
                _log_json("WARNING", "Instance failed processing.", instance_id=instance_id, error=error_message)
            else:
                probabilities = F.softmax(inference_output_tensor.unsqueeze(0), dim=1) 
                predicted_class = torch.argmax(probabilities, dim=1).item()
                
                is_detected = (predicted_class == 1)
                confidence = probabilities[0][predicted_class].item()

                results.append({
                    "instance_id": instance_id,
                    "detected": bool(is_detected),
                    "confidence": float(confidence),
                    "detection_details": "Fire detected by AI model" if is_detected else "No fire detected by AI model",
                    "error_message": None
                })
        
        _log_json("INFO", "Postprocessing complete.", total_results_generated=len(results))
        return results

# --- Example Usage (for local testing of handler methods, not full TorchServe) ---
if __name__ == '__main__':
    from src.common.config import GCS_BUCKET_NAME 

    _log_json("INFO", "Running local handler test (partial - requires GCS setup and model file).")

    class MockContext:
        def __init__(self, model_dir, gpu_id=None):
            self.system_properties = {"model_dir": model_dir, "gpu_id": gpu_id}
            self.manifest = {"model": {"modelName": "model"}}

    if not os.path.exists("model_artifacts"):
        os.makedirs("model_artifacts")
    
    dummy_model_for_test = DummyFireDetectionModel()
    torch.save(dummy_model_for_test.state_dict(), os.path.join("model_artifacts", "model.pth"))
    _log_json("INFO", "Dummy model file 'model.pth' created for local handler test.")

    mock_context = MockContext("model_artifacts")

    handler = FireDetectionHandler()
    try:
        handler.initialize(mock_context)

        valid_gcs_uri = f"gs://{GCS_BUCKET_NAME}/raw_satellite_imagery/test_image_valid.png" 
        invalid_gcs_uri = f"gs://{GCS_BUCKET_NAME}/raw_satellite_imagery/non_existent_image.png"
        malformed_gcs_uri = "gs:/malformed/uri"
        missing_uri_input = {"instance_id": "test_instance_4_missing_uri", "region_metadata": {"id": "test_region_4"}}
        
        dummy_local_image_path = "dummy_handler_test_image.png"
        if not os.path.exists(dummy_local_image_path):
            Image.new('RGB', (224, 224), color = 'green').save(dummy_local_image_path)
            _log_json("INFO", f"Created dummy local image for GCS upload test: {dummy_local_image_path}")
        
        try:
            if GCS_BUCKET_NAME != "fire-app-bucket":
                 _log_json("WARNING", f"GCS_BUCKET_NAME '{GCS_BUCKET_NAME}' might not be intended for this test. Skipping GCS upload.")
            else:
                storage_client_test = storage.Client()
                bucket_test = storage_client_test.bucket(GCS_BUCKET_NAME)
                blob_test = bucket_test.blob(valid_gcs_uri.replace(f"gs://{GCS_BUCKET_NAME}/", ""))
                if not blob_test.exists():
                    blob_test.upload_from_filename(dummy_local_image_path)
                    _log_json("INFO", f"Uploaded {dummy_local_image_path} to {valid_gcs_uri} for testing.")
                else:
                    _log_json("INFO", f"Test image {valid_gcs_uri} already exists in GCS.")
        except Exception as e:
            _log_json("WARNING", f"Could not ensure test image exists at {valid_gcs_uri} due to: {e}. GCS test case might fail.")

        input_batch_for_preprocess = [
            {"instance_id": "test_instance_1_valid", "gcs_image_uri": valid_gcs_uri, "region_metadata": {"id": "test_region_1"}},
            {"instance_id": "test_instance_2_invalid_uri", "gcs_image_uri": invalid_gcs_uri, "region_metadata": {"id": "test_region_2"}},
            {"instance_id": "test_instance_3_malformed_uri", "gcs_image_uri": malformed_gcs_uri, "region_metadata": {"id": "test_region_3"}},
            missing_uri_input,
            {"instance_id": "test_instance_5_another_valid", "gcs_image_uri": valid_gcs_uri, "region_metadata": {"id": "test_region_5"}} 
        ]
        
        _log_json("INFO", "--- Testing Preprocess ---")
        processed_tensors, metadata_list = handler.preprocess(input_batch_for_preprocess)
        
        for i, tensor in enumerate(processed_tensors):
            meta = metadata_list[i] if i < len(metadata_list) else {"instance_id": "error_meta_missing", "preprocess_error": "Metadata missing"}
            status = "Success" if tensor is not None else f"Failure ({meta.get('preprocess_error', 'Unknown preprocess error')})"
            _log_json("INFO", f"Preprocess instance {meta.get('instance_id')}: {status}")

        _log_json("INFO", "--- Testing Inference ---")
        inference_outputs, final_metadata_list = handler.inference((processed_tensors, metadata_list))

        _log_json("INFO", "--- Testing Postprocess ---")
        final_results = handler.postprocess((inference_outputs, final_metadata_list))
        
        _log_json("INFO", "Final batch results (should include errors for failed instances):")
        for res_idx, res_item in enumerate(final_results): # Changed variable name from res to res_item
            print(f"Result for instance {res_idx}: {json.dumps(res_item, indent=2)}") # Added index for clarity

    except Exception as e:
        _log_json("CRITICAL", f"Unhandled error during handler local test: {e}", error_details=str(e))
        import traceback
        traceback.print_exc()
</file>

<file path="register_vertex_model.py">
# register_vertex_model.py
from google.cloud import aiplatform
import os

PROJECT_ID = "haryo-kebakaran"
REGION = "asia-southeast2"
BUCKET_NAME = "fire-app-bucket"

aiplatform.init(project=PROJECT_ID, location=REGION)

serving_container_image_uri = f"{REGION}-docker.pkg.dev/{PROJECT_ID}/wildfire-detector-ts/wildfire-detector-ts:latest"
artifact_uri = f"gs://{BUCKET_NAME}/models/dummy_fire_detection/"

try:
    print(f"Attempting to upload model '{PROJECT_ID}' in '{REGION}'...")
    model = aiplatform.Model.upload(
        display_name="dummy_wildfire_detector_v1",
        artifact_uri=artifact_uri,
        serving_container_image_uri=serving_container_image_uri,
        serving_container_predict_route="/predictions/model",
        serving_container_health_route="/ping"
    )
    model.wait()
    print(f"Model uploaded successfully. Resource name: {model.resource_name}")
    # IMPORTANT: Copy the NEW MODEL ID from the end of the 'resource_name' output.
except Exception as e:
    print(f"Error uploading model: {e}")
    print("This might happen if the model with this display_name already exists.")
    print("If it exists, check Vertex AI -> Models to get its ID, or increment version.")
</file>

<file path="requirements.txt">
# src/ml_model/requirements.txt (for both Dockerfile and .mar bundle)
google-cloud-storage
Pillow
</file>

</files>
</file>

<file path="src/ml_model/requirements.txt">
# src/ml_model/requirements.txt (for both Dockerfile and .mar bundle)
google-cloud-storage
Pillow
</file>

<file path=".gitignore">
# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
venv/
env/
fire-venv/
*.egg-info/
.pytest_cache/
.coverage
.mypy_cache/
.vscode/

# Environment Variables
.env
.mock_fire_state # From firms_api mock
/tmp/ # Temporary directory used on VM and locally
</file>

<file path="archive_model.sh">
#!/bin/bash

# Ensure you are in the project root directory (e.g., fire-detection-app/)
# cd /path/to/your/fire-detection-app

echo "Step 1: Generating model.pth..."
# Run the python script to save model.pth into src/ml_model/
python src/ml_model/fire_detection_model.py

# Check if model.pth was created successfully
if [ ! -f "src/ml_model/model.pth" ]; then
    echo "ERROR: src/ml_model/model.pth not found after running fire_detection_model.py!"
    exit 1
fi
echo "src/ml_model/model.pth generated successfully."

echo "Step 2: Archiving model into model.mar..."
# Define a temporary directory for torch-model-archiver output
TEMP_EXPORT_PATH="src/ml_model/model_store_temp"
mkdir -p "$TEMP_EXPORT_PATH" # Create if it doesn't exist

# Run torch-model-archiver
# Paths are relative to the project root where this script is run.
torch-model-archiver --model-name model \
                     --version 1.0 \
                     --model-file src/ml_model/fire_detection_model.py \
                     --serialized-file src/ml_model/model.pth \
                     --handler src/ml_model/handler.py \
                     --requirements-file src/ml_model/requirements.txt \
                     --export-path "$TEMP_EXPORT_PATH" \
                     --force

# Check if model.mar was created in the temp export path
if [ ! -f "$TEMP_EXPORT_PATH/model.mar" ]; then
    echo "ERROR: $TEMP_EXPORT_PATH/model.mar not found after running torch-model-archiver!"
    rm -rf "$TEMP_EXPORT_PATH" # Clean up temp directory on failure
    exit 1
fi
echo "$TEMP_EXPORT_PATH/model.mar created successfully."

echo "Step 3: Moving model.mar to its final location for Docker..."
# Move the created model.mar to the location the Dockerfile expects
mv "$TEMP_EXPORT_PATH/model.mar" "src/ml_model/model.mar"

# Check if the move was successful
if [ ! -f "src/ml_model/model.mar" ]; then
    echo "ERROR: Failed to move model.mar to src/ml_model/model.mar!"
    rm -rf "$TEMP_EXPORT_PATH" # Clean up temp directory
    exit 1
fi
echo "model.mar moved to src/ml_model/model.mar."

echo "Step 4: Cleaning up temporary export directory..."
rm -rf "$TEMP_EXPORT_PATH"
echo "Temporary directory $TEMP_EXPORT_PATH removed."

echo "Model archiving process completed successfully."
echo "You can now build your Docker image."
</file>

<file path="cloudbuild-pipeline-initiator.yaml">
# fire-detection-app/cloudbuild-pipeline-initiator.yaml
steps:
# Step 1: Create a staging directory for the function
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    mkdir -p deploy_staging/pipeline_initiator
    cp src/cloud_functions/pipeline_initiator/main.py deploy_staging/pipeline_initiator/main.py
    cp src/cloud_functions/pipeline_initiator/requirements.txt deploy_staging/pipeline_initiator/requirements.txt
    # Copy the entire shared 'src' directory into the staging area for this function
    # This makes 'from src.common...' imports work.
    cp -r src deploy_staging/pipeline_initiator/src

# Step 2: Deploy the function from the staging directory
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk' # Official Cloud SDK image
  entrypoint: 'gcloud'
  args:
  - 'functions'
  - 'deploy'
  - 'PipelineInitiatorCF'
  - '--gen2'
  - '--runtime=python311'
  - '--region=asia-southeast2'
  - '--source=./deploy_staging/pipeline_initiator' # Source is now the prepared staging dir
  - '--entry-point=pipeline_initiator_cloud_function'
  - '--trigger-topic=wildfire-pipeline-initiator'
  - '--service-account=fire-app-vm-service-account@haryo-kebakaran.iam.gserviceaccount.com'
  - '--set-env-vars=GCP_PROJECT_ID=haryo-kebakaran,GCP_REGION=asia-southeast2,FIRMS_API_KEY=0331973a7ee830ca7f026493faaa367a,VERTEX_AI_MODEL_NAME=dummy_wildfire_detector_v1,VERTEX_NOTIFICATION_PUBSUB_TOPIC_NAME=vertex-job-completion-topic'
  - '--timeout=540s'
  - '--memory=1GiB'  # <--- INCREASED MEMORY
  - '--project=haryo-kebakaran'

options:
  logging: CLOUD_LOGGING_ONLY
  # machineType: 'E2_MEDIUM' # Optional: specify machine type for the build itself
timeout: '1200s' # Timeout for the entire Cloud Build job (all steps)
</file>

<file path="cloudbuild-result-processor.yaml">
# fire-detection-app/cloudbuild-result-processor.yaml
steps:
# Step 1: Create a staging directory for the function
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    mkdir -p deploy_staging/result_processor
    cp src/cloud_functions/result_processor/main.py deploy_staging/result_processor/main.py
    cp src/cloud_functions/result_processor/requirements.txt deploy_staging/result_processor/requirements.txt
    # Copy the entire shared 'src' directory
    cp -r src deploy_staging/result_processor/src

# Step 2: Deploy the function from the staging directory
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args:
  - 'functions'
  - 'deploy'
  - 'ResultProcessorCF'
  - '--gen2'
  - '--runtime=python311'
  - '--region=asia-southeast2'
  - '--source=./deploy_staging/result_processor' # Source is the prepared staging dir
  - '--entry-point=result_processor_cloud_function'
  - '--trigger-topic=vertex-job-completion-topic'
  - '--service-account=fire-app-vm-service-account@haryo-kebakaran.iam.gserviceaccount.com'
  - '--set-env-vars=GCP_PROJECT_ID=haryo-kebakaran,FIRMS_API_KEY=0331973a7ee830ca7f026493faaa367a,GCP_REGION=asia-southeast2'
  - '--timeout=540s'
  - '--memory=1024MB'
  - '--project=haryo-kebakaran'

options:
  logging: CLOUD_LOGGING_ONLY
  # machineType: 'E2_MEDIUM'
timeout: '1200s'
</file>

<file path="cloudbuild.yaml">
# cloudbuild.yaml
steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [
    'build',
    '-t', 'asia-southeast2-docker.pkg.dev/haryo-kebakaran/wildfire-detector-ts/wildfire-detector-ts:latest',
    '-f', 'src/ml_model/Dockerfile', # Path to your Dockerfile from project root
    '.'  # Build context is the project root (where this cloudbuild.yaml is)
  ]
images:
- 'asia-southeast2-docker.pkg.dev/haryo-kebakaran/wildfire-detector-ts/wildfire-detector-ts:latest' # This ensures the image is pushed
options:
  machineType: 'E2_HIGHCPU_8' # Using E2_HIGHCPU_8 (8 vCPUs, 8 GB RAM)
timeout: '1800s' # 30 minutes (should be more than enough)
</file>

<file path="src/common/config.py">
# src/common/config.py

"""
Global configuration settings for the Wildfire Detection System.
"""

# Define the geographic areas to monitor for wildfires.
# Each region is a dictionary with:
# - 'id': A unique identifier for the region.
# - 'name': A human-readable name for the region.
# - 'bbox': A bounding box defined as [min_longitude, min_latitude, max_longitude, max_latitude].
# - 'description': A brief description of the region.
MONITORED_REGIONS = [
    {
        "id": "sierra_nf_section", # New ID for the smaller region
        "name": "Sierra National Forest Section (1x1 Test)",
        "bbox": [-119.5, 37.0, -118.5, 38.0],  # [min_lon, min_lat, max_lon, max_lat]
        "description": "A small 1x1 degree test region in Sierra National Forest, California."
    }
    # {
    #     "id": "california_central", # Keeping original commented out for reference
    #     "name": "Central California Wildfire Zone",
    #     "bbox": [-122.0, 36.0, -118.0, 38.0],
    #     "description": "A test region in Central California prone to wildfires."
    # },
    # { # Temporarily commented out for faster debugging
    #     "id": "australia_southeast",
    #     "name": "Southeast Australia Bushfire Zone",
    #     "bbox": [140.0, -38.0, 153.0, -28.0],
    #     "description": "A test region in Southeast Australia."
    # }
    # Add more regions as needed
]

# Your Google Cloud Storage bucket name.
# This bucket will be used to store raw data, processed outputs (maps, metadata),
# and Vertex AI batch prediction inputs/outputs.
GCS_BUCKET_NAME = "fire-app-bucket" # Ensure this is your correct bucket name
</file>

<file path="src/firms_data_retriever/retriever.py">
# src/firms_data_retriever/retriever.py

import os
import logging
import requests
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import json # For _log_json helper

# Import MONITORED_REGIONS from the common config file
from src.common.config import MONITORED_REGIONS

# --- Configuration ---
FIRMS_API_KEY = os.environ.get("FIRMS_API_KEY")
FIRMS_API_BASE_URL = "https://firms.modaps.eosdis.nasa.gov/api/area/csv/"
FIRMS_SENSORS = ["VIIRS_SNPP_NRT", "VIIRS_NOAA20_NRT"]

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def _log_json(severity: str, message: str, **kwargs):
    """
    Helper to log structured JSON messages to stdout, which GCP Cloud Logging
    can ingest as structured logs.
    """
    log_entry = {
        "severity": severity.upper(),
        "message": message,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "component": "FirmsDataRetriever",
        **kwargs
    }
    print(json.dumps(log_entry))


class FirmsDataRetriever:
    """
    Component 1: Fetches active fire data from NASA FIRMS API (using /api/area/)
    and filters it for predefined monitored regions.
    """

    def __init__(self, api_key: str, base_url: str, sensors: List[str]):
        """
        Initializes the FIRMS data retriever.
        """
        if not api_key:
            _log_json("CRITICAL", "FIRMS_API_KEY environment variable not set. Cannot proceed.")
            raise ValueError("FIRMS_API_KEY is required for FirmsDataRetriever.")
        self.api_key = api_key
        self.base_url = base_url
        self.sensors = sensors
        _log_json("INFO", "FirmsDataRetriever initialized.", sensors=self.sensors, base_url=self.base_url)

    def _fetch_firms_data(self, sensor: str) -> Optional[pd.DataFrame]:
        """
        Fetches FIRMS data for a specific sensor for the last 24 hours (yesterday's data).
        Uses the /api/area/ endpoint.
        Includes debug print for raw CSV.
        """
        yesterday = datetime.utcnow() - timedelta(days=1)
        date_str = yesterday.strftime('%Y-%m-%d')
        day_range = "1"

        endpoint = f"{self.base_url}{self.api_key}/{sensor}/world/{day_range}/{date_str}"
        _log_json("INFO", "Attempting to fetch FIRMS data using /api/area/.",
                  sensor=sensor, date_for_data=date_str, endpoint=endpoint)

        try:
            response = requests.get(endpoint, timeout=60) # Increased timeout

            if "Invalid API call" in response.text:
                _log_json("ERROR", "FIRMS API (/api/area/) returned 'Invalid API call'. Check API key, sensor name, or endpoint structure.",
                          api_response_snippet=response.text[:200], status_code=response.status_code, sensor=sensor, endpoint=endpoint)
                return None

            response.raise_for_status()

            if not response.text.strip() or response.text.startswith("No fire data found"):
                _log_json("WARNING", "No fire data found or empty response from FIRMS API for query.",
                          sensor=sensor, endpoint=endpoint, response_text_snippet=response.text[:100])
                return None

            # --- DEBUG PRINT START ---
            print(f"\n--- RAW FIRMS CSV Output (first 5 lines) for sensor: {sensor} ---")
            lines = response.text.splitlines()
            for i in range(min(5, len(lines))): # Print header + up to 4 data lines
                print(lines[i])
            print("--- END RAW FIRMS CSV Output ---\n")
            # --- DEBUG PRINT END ---

            if "Error" in response.text or "Access Denied" in response.text:
                _log_json("ERROR", "FIRMS API returned an error message in response body despite 200 OK.",
                          api_response_snippet=response.text[:200], status_code=response.status_code, sensor=sensor)
                return None

            df = pd.read_csv(pd.io.common.StringIO(response.text))
            _log_json("INFO", "Successfully fetched FIRMS data for sensor using /api/area/.",
                      sensor=sensor, rows_fetched=len(df))
            return df
        except requests.exceptions.HTTPError as e:
            _log_json("ERROR", "HTTP error fetching FIRMS data (/api/area/).",
                      error=str(e), status_code=e.response.status_code if e.response else 'N/A', sensor=sensor)
            return None
        except requests.exceptions.ConnectionError as e:
            _log_json("ERROR", "Connection error fetching FIRMS data (/api/area/).", error=str(e), sensor=sensor)
            return None
        except requests.exceptions.Timeout as e:
            _log_json("ERROR", "Timeout fetching FIRMS data (/api/area/).", error=str(e), sensor=sensor)
            return None
        except requests.exceptions.RequestException as e:
            _log_json("ERROR", "An unexpected requests error occurred (/api/area/).", error=str(e), sensor=sensor)
            return None
        except pd.errors.EmptyDataError:
            _log_json("WARNING", "FIRMS CSV data is empty or malformed after successful fetch (pd.errors.EmptyDataError).", sensor=sensor)
            return None
        except Exception as e:
            _log_json("ERROR", "An unexpected error occurred during FIRMS data fetch or parsing (/api/area/).",
                      error_type=type(e).__name__, error=str(e), sensor=sensor)
            return None

    def get_and_filter_firms_data(self, monitored_regions: List[Dict[str, Any]]) -> pd.DataFrame:
        """
        Fetches FIRMS data for all specified sensors (for yesterday)
        and filters it by the provided monitored regions.
        """
        all_firms_data = []

        for sensor in self.sensors:
            df_sensor = self._fetch_firms_data(sensor)
            if df_sensor is not None and not df_sensor.empty:
                all_firms_data.append(df_sensor)

        empty_df_for_return = pd.DataFrame(columns=[
            'latitude', 'longitude', 'acq_date', 'acq_time', 'confidence',
            'frp', 'daynight', 'satellite', 'monitored_region_id'
        ])

        if not all_firms_data:
            _log_json("WARNING", "No FIRMS data retrieved from any sensor after attempts using /api/area/.")
            return empty_df_for_return

        combined_df = pd.concat(all_firms_data, ignore_index=True)
        initial_rows = len(combined_df)
        _log_json("INFO", "Combined raw FIRMS data from all sensors.", total_hotspots_before_filter=initial_rows)

        # Ensure required columns exist, even if they are all None initially from some sensors.
        # The FIRMS /api/area/ CSV should have these columns:
        # latitude,longitude,bright_ti4,scan,track,acq_date,acq_time,satellite,instrument,confidence,version,bright_ti5,frp,daynight
        # We are interested in: latitude, longitude, acq_date, acq_time, satellite, confidence, frp, daynight
        required_cols = [
            'latitude', 'longitude', 'acq_date', 'acq_time', 'confidence',
            'frp', 'daynight', 'satellite'
        ]
        for col in required_cols:
            if col not in combined_df.columns:
                _log_json("WARNING", f"Missing expected column in combined FIRMS data: {col}. Adding with None.", column=col)
                combined_df[col] = None

        if 'confidence' in combined_df.columns:
            combined_df['confidence'] = combined_df['confidence'].astype(str).str.lower()
            
            # Filter using the single-letter codes 'h' (high) and 'n' (nominal)
            confidence_values_to_keep = ['h', 'n']
            filtered_by_confidence_df = combined_df[
                combined_df['confidence'].isin(confidence_values_to_keep)
            ].copy()
            
            _log_json("INFO", "Filtered FIRMS data by confidence (kept 'h' or 'n').",
                      original_rows_before_confidence_filter=initial_rows,
                      rows_after_confidence_filter=len(filtered_by_confidence_df),
                      confidence_values_kept=confidence_values_to_keep)
        else:
            _log_json("WARNING", "No 'confidence' column found in FIRMS data. Skipping confidence filter.")
            filtered_by_confidence_df = combined_df.copy()

        if filtered_by_confidence_df.empty:
            _log_json("INFO", "No FIRMS hotspots with 'h' or 'n' confidence after filtering.")
            return empty_df_for_return

        filtered_hotspots_by_region = []
        for region in monitored_regions:
            region_id = region["id"]
            min_lon, min_lat, max_lon, max_lat = region["bbox"]

            # Create a working copy for this region's filter to avoid SettingWithCopyWarning
            temp_df_for_region_filter = filtered_by_confidence_df.copy()
            
            # Ensure latitude and longitude are numeric before filtering
            temp_df_for_region_filter['latitude'] = pd.to_numeric(temp_df_for_region_filter['latitude'], errors='coerce')
            temp_df_for_region_filter['longitude'] = pd.to_numeric(temp_df_for_region_filter['longitude'], errors='coerce')
            
            # Drop rows where lat/lon could not be coerced to numeric, as they can't be spatially filtered
            temp_df_for_region_filter.dropna(subset=['latitude', 'longitude'], inplace=True)

            region_df = temp_df_for_region_filter[
                (temp_df_for_region_filter['latitude'] >= min_lat) &
                (temp_df_for_region_filter['latitude'] <= max_lat) &
                (temp_df_for_region_filter['longitude'] >= min_lon) &
                (temp_df_for_region_filter['longitude'] <= max_lon)
            ].copy() # .copy() here ensures region_df is a new DataFrame

            if not region_df.empty:
                region_df['monitored_region_id'] = region_id
                filtered_hotspots_by_region.append(region_df)
                _log_json("INFO", "Found FIRMS hotspots in monitored region.", region_id=region_id, count=len(region_df))
            else:
                _log_json("INFO", "No FIRMS hotspots found in monitored region.", region_id=region_id)

        if not filtered_hotspots_by_region:
            _log_json("INFO", "No FIRMS hotspots found across all monitored regions after spatial filtering.")
            return empty_df_for_return

        final_df = pd.concat(filtered_hotspots_by_region, ignore_index=True)

        output_columns = [
            'latitude', 'longitude', 'acq_date', 'acq_time', 'confidence',
            'frp', 'daynight', 'satellite', 'monitored_region_id'
        ]
        # Ensure all output columns exist, fill with None if not
        for col in output_columns:
            if col not in final_df.columns:
                final_df[col] = None
        
        final_df = final_df[output_columns] # Reorder and select

        # Ensure correct data types for final output
        final_df['latitude'] = pd.to_numeric(final_df['latitude'], errors='coerce')
        final_df['longitude'] = pd.to_numeric(final_df['longitude'], errors='coerce')
        final_df['frp'] = pd.to_numeric(final_df['frp'], errors='coerce')
        
        final_df['acq_date'] = pd.to_datetime(final_df['acq_date'], errors='coerce').dt.strftime('%Y-%m-%d')
        # Ensure acq_time is string, remove potential '.0' from float conversion, then zfill
        final_df['acq_time'] = final_df['acq_time'].astype(str).str.replace(r'\.0$', '', regex=True).str.zfill(4)
        
        final_df['confidence'] = final_df['confidence'].astype(str) # Already lowercased
        final_df['daynight'] = final_df['daynight'].astype(str)
        final_df['satellite'] = final_df['satellite'].astype(str)
        final_df['monitored_region_id'] = final_df['monitored_region_id'].astype(str)

        _log_json("INFO", "FIRMS data retrieval and filtering complete using /api/area/.",
                  total_filtered_hotspots=len(final_df))
        return final_df

# --- Example Usage (for local testing) ---
if __name__ == "__main__":
    if "FIRMS_API_KEY" not in os.environ:
        print("WARNING: FIRMS_API_KEY environment variable not set for local testing.")
        # Replace with your actual key if you want to run this block successfully
        # Example: os.environ["FIRMS_API_KEY"] = "YOUR_ACTUAL_FIRMS_KEY"
        # For this test, let's assume it's set or use a placeholder that might fail
        os.environ["FIRMS_API_KEY"] = os.environ.get("FIRMS_API_KEY", "0331973a7ee830ca7f026493faaa367a") # Using your key or placeholder
        if os.environ["FIRMS_API_KEY"] == "YOUR_DUMMY_KEY_FOR_LOCAL_TESTING":
             _log_json("WARNING", "Using placeholder FIRMS_API_KEY. Local test might fail or return 'Invalid API call'.")

    try:
        firms_retriever = FirmsDataRetriever(
            api_key=FIRMS_API_KEY,
            base_url=FIRMS_API_BASE_URL,
            sensors=FIRMS_SENSORS
        )

        _log_json("INFO", "Starting FIRMS data retrieval and filtering process (local test)...")
        # MONITORED_REGIONS is imported from src.common.config
        filtered_firms_data = firms_retriever.get_and_filter_firms_data(MONITORED_REGIONS)

        _log_json("INFO", "Filtered FIRMS Hotspots Data (first 5 rows if any):")
        print(filtered_firms_data.head().to_string()) # .to_string() for better console output
        _log_json("INFO", f"Total filtered hotspots: {len(filtered_firms_data)}")

        if not filtered_firms_data.empty:
            hotspots_by_region = filtered_firms_data.groupby('monitored_region_id').size().reset_index(name='hotspot_count')
            _log_json("INFO", "Hotspots per Monitored Region:")
            print(hotspots_by_region.to_string())
        else:
            _log_json("INFO", "No hotspots to group by region.")

    except ValueError as e:
        _log_json("ERROR", f"Configuration Error during local test: {e}")
    except Exception as e:
        _log_json("CRITICAL", "An unhandled error occurred during FIRMS local test.",
                  error_type=type(e).__name__, error=str(e))
        import traceback
        traceback.print_exc()
</file>

<file path="src/ml_model/Dockerfile">
# src/ml_model/Dockerfile
FROM pytorch/pytorch:2.7.0-cuda11.8-cudnn9-runtime

LABEL maintainer="Wildfire Detection Team <dev@example.com>"
LABEL version="0.1.0"
LABEL description="TorchServe container for Wildfire Detection AI Model (PyTorch 2.7.0)"

ENV PATH="/opt/conda/bin:${PATH}"
ENV HOME="/home/model-server"
ENV PYTHONUNBUFFERED=TRUE
ENV TEMP_DIR_PATH="/tmp"

WORKDIR ${HOME}

RUN apt-get update && \
    apt-get install -y --no-install-recommends openjdk-11-jdk-headless curl wget && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir --upgrade pip

COPY src/ml_model/requirements.txt .

RUN pip install --no-cache-dir --default-timeout=120 --retries=3 \
    "torchserve==0.12.0" \
    "torch-model-archiver==0.12.0" \
    -r requirements.txt

COPY src/ml_model/model.mar ./model_store/model.mar

EXPOSE 8080
EXPOSE 8081
EXPOSE 8082

HEALTHCHECK --interval=30s --timeout=10s --start-period=300s --retries=3 \
  CMD curl -f http://localhost:8080/ping || exit 1

ENTRYPOINT ["torchserve"]
CMD ["--start", "--model-store", "/home/model-server/model_store", "--models", "model=model.mar", "--disable-token-authorization"]
</file>

<file path="src/ml_model/fire_detection_model.py">
# src/ml_model/fire_detection_model.py

import os # Added os import for path joining
import torch
import torch.nn as nn
from torchvision.transforms import Compose, Resize, ToTensor, Normalize

class DummyFireDetectionModel(nn.Module):
    """
    A dummy PyTorch model for wildfire detection.
    For MVP, it simply acts as a placeholder and always returns a predefined "detected" state.
    In a real scenario, this would be a trained Convolutional Neural Network (CNN).
    """
    def __init__(self):
        super(DummyFireDetectionModel, self).__init__()
        # For a real model, you'd define your layers here, e.g.:
        # self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        # self.relu = nn.ReLU()
        # self.pool = nn.MaxPool2d(2)
        # self.fc = nn.Linear(..., 2) # e.g., for binary classification (fire/no-fire)

        # We'll use a placeholder parameter so TorchServe finds something to load
        self.dummy_param = nn.Parameter(torch.randn(1))

    def forward(self, x):
        """
        Forward pass for the dummy model.
        In a real model, x would be an image tensor, and you'd pass it through your layers.
        For this dummy, we ignore the input and return a fixed output.
        """
        # For MVP, simulate a fixed output: assume fire detected with 85% confidence
        # Real model would output logits or probabilities, e.g., torch.tensor([[0.15, 0.85]])
        return torch.tensor([[0.15, 0.85]]) # [no_fire_prob, fire_prob]

# Define a transformation pipeline that mimics what a real model would need for input.
# This will be used in the handler's preprocess method.
# Assuming input images are 224x224 RGB for a typical CNN.
MODEL_INPUT_TRANSFORMS = Compose([
    Resize((224, 224)),       # Resize to expected input size
    ToTensor(),               # Convert PIL Image to PyTorch Tensor (HWC to CHW, 0-255 to 0-1)
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization (common)
])

if __name__ == '__main__':
    # Simple test of the dummy model
    model = DummyFireDetectionModel()
    dummy_input = torch.randn(1, 3, 224, 224) # Batch size 1, 3 channels, 224x224 image
    output = model(dummy_input)
    print("Dummy Model Output (logits/probabilities):", output)
    # Convert logits/probabilities to predicted class (0 for no fire, 1 for fire)
    predicted_class = torch.argmax(output, dim=1).item()
    confidence = output.softmax(dim=1)[0][predicted_class].item()
    print(f"Predicted Class: {'Fire' if predicted_class == 1 else 'No Fire'} (Confidence: {confidence:.2f})")

    # --- MODIFIED SAVE LOGIC ---
    # Save a dummy state_dict for packaging, named 'model.pth'
    # and place it directly within the 'src/ml_model/' directory.
    model_filename = "model.pth"
    # Get the directory where this script (fire_detection_model.py) is located
    current_script_directory = os.path.dirname(os.path.abspath(__file__))
    save_path = os.path.join(current_script_directory, model_filename)
    
    torch.save(model.state_dict(), save_path)
    print(f"Dummy model state_dict saved to {save_path}")
    # --- END MODIFIED SAVE LOGIC ---
</file>

<file path=".gcloudignore">
# .gcloudignore
.gcloudignore
.git/
.gitignore
__pycache__/
*.py[cod]
*$py.class
*~
fire-venv/
fire-venv

# Or whatever your virtual environment folder is named  <--- THIS IS KEY

# Optional: Add other files/directories you don't want to deploy
# For example, local test outputs or large data files not needed by the function
*.test
*.png # If you have test images at the root you don't want to deploy
dummy_*.pth
# node_modules/ (if you had any)
# .vscode/
# .idea/
</file>

<file path="requirements.txt">
requests
pandas
google-cloud-storage
torch
torchvision
torchserve
torch-model-archiver
Pillow
</file>

<file path="src/cloud_functions/pipeline_initiator/main.py">
# src/cloud_functions/pipeline_initiator/main.py

import os
import logging
import json
from datetime import datetime, timedelta, timezone as dt_timezone

from google.cloud import storage
from google.cloud import aiplatform # Main SDK client initialization

# DIRECTLY import the types we need from their specific modules
from google.cloud.aiplatform_v1.services.job_service import JobServiceClient
from google.cloud.aiplatform_v1.types.batch_prediction_job import BatchPredictionJob
from google.cloud.aiplatform_v1.types.io import GcsDestination, GcsSource
from google.cloud.aiplatform_v1.types.machine_resources import MachineSpec, BatchDedicatedResources

from typing import Dict, Any, List
import base64 # For local testing Pub/Sub message encoding

# --- Logging Setup ---
logger_global = logging.getLogger(__name__)

def _log_json(severity: str, message: str, **kwargs):
    log_entry = {
        "severity": severity.upper(),
        "message": message,
        "timestamp": datetime.now(dt_timezone.utc).isoformat(),
        "component": "PipelineInitiatorCF",
        **kwargs
    }
    print(json.dumps(log_entry))
# --- End Logging Setup ---

# Import components from our project structure
# Ensure these imported files are also the latest corrected versions
from src.common.config import MONITORED_REGIONS, GCS_BUCKET_NAME
from src.firms_data_retriever.retriever import FirmsDataRetriever
from src.satellite_imagery_acquirer.acquirer import SatelliteImageryAcquirer


# --- Configuration Constants ---
GCS_BATCH_INPUT_DIR = "vertex_ai_batch_inputs/"
GCS_BATCH_OUTPUT_DIR_PREFIX = "vertex_ai_batch_outputs/"


def _get_vertex_ai_model_resource_name(model_display_name: str, project_id: str, location: str) -> str:
    """
    Retrieves the full resource name for the latest version of a Vertex AI model
    based on its display name.
    """
    try:
        # aiplatform.init already called globally if needed, or can be called here too.
        # For safety, ensure it's initialized for the context of this function.
        aiplatform.init(project=project_id, location=location)
        models = aiplatform.Model.list(
            filter=f'display_name="{model_display_name}"',
            order_by="update_time desc" # Get the latest version
        )
        if not models:
            err_msg = f"Vertex AI Model with display_name '{model_display_name}' not found in project '{project_id}', region '{location}'."
            _log_json("ERROR", err_msg, model_name=model_display_name, project_id=project_id, location=location)
            raise ValueError(err_msg)

        latest_model = models[0]
        _log_json("INFO", "Found Vertex AI Model.",
                   model_display_name=model_display_name,
                   resource_name=latest_model.resource_name,
                   version_id=latest_model.version_id,
                   update_time=latest_model.update_time.isoformat() if latest_model.update_time else "N/A")
        return latest_model.resource_name
    except Exception as e:
        _log_json("CRITICAL", f"Failed to retrieve Vertex AI model resource name for '{model_display_name}'.",
                   model_display_name=model_display_name, error=str(e), error_type=type(e).__name__)
        raise


# Main Cloud Function entry point
def pipeline_initiator_cloud_function(event: Dict, context: Dict):
    _log_json("INFO", "Pipeline Initiator Cloud Function triggered.",
               event_id=context.event_id if hasattr(context, 'event_id') else 'N/A',
               event_type=context.event_type if hasattr(context, 'event_type') else 'N/A',
               trigger_resource=context.resource.get("name") if hasattr(context, 'resource') and isinstance(context.resource, dict) else str(getattr(context, 'resource', 'N/A')),
               timestamp=context.timestamp if hasattr(context, 'timestamp') else 'N/A')

    # --- Configuration & Validation ---
    gcp_project_id = os.environ.get("GCP_PROJECT_ID")
    gcp_region = os.environ.get("GCP_REGION")
    firms_api_key = os.environ.get("FIRMS_API_KEY")
    vertex_ai_model_name = os.environ.get("VERTEX_AI_MODEL_NAME", "dummy_wildfire_detector_v1")
    # VERTEX_NOTIFICATION_PUBSUB_TOPIC_NAME is no longer used by this function to configure the job directly.
    # It will be the target for the Cloud Logging Sink.

    core_env_vars = {
        "GCP_PROJECT_ID": gcp_project_id, "GCP_REGION": gcp_region,
        "FIRMS_API_KEY": firms_api_key, "VERTEX_AI_MODEL_NAME": vertex_ai_model_name,
        "GCS_BUCKET_NAME (from config)": GCS_BUCKET_NAME
    }
    missing_vars = [var for var, val in core_env_vars.items() if not val]

    if missing_vars:
        _log_json("CRITICAL", "Missing one or more required environment variables or configurations. Cannot proceed.",
                   missing_variables=missing_vars)
        raise ValueError(f"Missing required configurations: {', '.join(missing_vars)}")

    _log_json("INFO", "Environment variables and configurations loaded.",
               project_id=gcp_project_id, region=gcp_region,
               gcs_bucket=GCS_BUCKET_NAME, model_name=vertex_ai_model_name,
               firms_key_present=bool(firms_api_key))
    _log_json("INFO", "Note: Direct Vertex AI job notifications (NotificationSpec) are NOT configured by this function "
                      "due to client library limitations. A Cloud Logging Sink should be used to forward job "
                      "completion events to the appropriate Pub/Sub topic for the ResultProcessorCF.")


    target_date_for_data = datetime.now(dt_timezone.utc) - timedelta(days=1) # Process yesterday's data
    acquisition_date_str = target_date_for_data.strftime('%Y-%m-%d')
    _log_json("INFO", f"Target acquisition date for FIRMS and Satellite Imagery: {acquisition_date_str}")

    try:
        # Initialize clients
        storage_client = storage.Client(project=gcp_project_id)
        # Initialize aiplatform client, explicitly setting project and location for clarity
        aiplatform.init(project=gcp_project_id, location=gcp_region)

        # --- 1. FIRMS Data Retrieval ---
        _log_json("INFO", "Initiating FIRMS data retrieval.")
        firms_retriever = FirmsDataRetriever(
            api_key=firms_api_key,
            base_url="https://firms.modaps.eosdis.nasa.gov/api/area/csv/",
            sensors=["VIIRS_SNPP_NRT", "VIIRS_NOAA20_NRT"]
        )
        firms_hotspots_df = firms_retriever.get_and_filter_firms_data(MONITORED_REGIONS)
        _log_json("INFO", "FIRMS data retrieval complete.", firms_hotspots_count=len(firms_hotspots_df))

        # --- 2. Satellite Imagery Acquisition ---
        _log_json("INFO", "Initiating satellite imagery acquisition (GEE export tasks).")
        imagery_acquirer = SatelliteImageryAcquirer(gcs_bucket_name=GCS_BUCKET_NAME)
        gcs_image_uris_by_region_id: Dict[str, str] = imagery_acquirer.acquire_and_export_imagery(
            MONITORED_REGIONS, acquisition_date=acquisition_date_str
        )
        _log_json("INFO", "Satellite imagery GEE export tasks initiation complete.",
                           image_tasks_initiated_count=len(gcs_image_uris_by_region_id))

        if not gcs_image_uris_by_region_id:
            _log_json("WARNING", "No satellite imagery export tasks were initiated. Skipping Vertex AI Batch Prediction.")
            return # Exit if no images to process

        # --- 3. Prepare Vertex AI Batch Prediction Input ---
        _log_json("INFO", "Preparing Vertex AI Batch Prediction input file (JSONL).")
        batch_input_instances: List[Dict[str, Any]] = []
        for region_id, gcs_image_uri in gcs_image_uris_by_region_id.items():
            region_metadata = next((r for r in MONITORED_REGIONS if r["id"] == region_id), None)
            if not region_metadata:
                _log_json("WARNING", f"Metadata for region_id '{region_id}' not found in MONITORED_REGIONS. Skipping instance.",
                                   region_id_from_imagery=region_id)
                continue
            
            region_firms_count = 0
            if not firms_hotspots_df.empty and 'monitored_region_id' in firms_hotspots_df.columns:
                region_specific_firms = firms_hotspots_df[firms_hotspots_df['monitored_region_id'] == region_id]
                region_firms_count = region_specific_firms.shape[0]

            # Unique instance ID for each image, incorporating date
            instance_id = f"{region_id}_{acquisition_date_str.replace('-', '')}"
            batch_input_instances.append({
                "instance_id": instance_id,
                "gcs_image_uri": gcs_image_uri,
                "region_metadata": region_metadata, # Include full region metadata
                "firms_hotspot_count_in_region": region_firms_count
            })

        if not batch_input_instances:
            _log_json("WARNING", "No valid instances prepared for Vertex AI Batch Prediction after filtering. Exiting.")
            return

        # Upload batch input file to GCS
        timestamp_str = datetime.now(dt_timezone.utc).strftime('%Y%m%d%H%M%S%f') # Microsecond precision for uniqueness
        input_filename = f"batch_input_{acquisition_date_str.replace('-', '')}_{timestamp_str}.jsonl"
        input_gcs_path_in_bucket = f"{GCS_BATCH_INPUT_DIR}{input_filename}"
        input_gcs_uri = f"gs://{GCS_BUCKET_NAME}/{input_gcs_path_in_bucket}"

        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob(input_gcs_path_in_bucket)
        jsonl_content = "\n".join([json.dumps(instance) for instance in batch_input_instances])
        blob.upload_from_string(jsonl_content, content_type="application/jsonl", timeout=60)
        _log_json("INFO", "Vertex AI Batch Prediction input file uploaded to GCS.",
                           input_gcs_uri=input_gcs_uri, num_instances=len(batch_input_instances))

        # --- 4. Submit Vertex AI Batch Prediction Job ---
        _log_json("INFO", "Constructing Vertex AI Batch Prediction job arguments.")
        model_resource_name = _get_vertex_ai_model_resource_name(vertex_ai_model_name, gcp_project_id, gcp_region)
        
        # Output prefix in GCS for this job's results
        batch_job_output_gcs_prefix = f"gs://{GCS_BUCKET_NAME}/{GCS_BATCH_OUTPUT_DIR_PREFIX}"
        
        job_display_name = f"wildfire_detection_batch_{acquisition_date_str.replace('-', '')}_{timestamp_str}"
        
        input_config = BatchPredictionJob.InputConfig(
            instances_format="jsonl",
            gcs_source=GcsSource(uris=[input_gcs_uri])
        )
        output_config = BatchPredictionJob.OutputConfig(
            predictions_format="jsonl", # Output from handler will be JSONL
            gcs_destination=GcsDestination(output_uri_prefix=batch_job_output_gcs_prefix)
        )
        
        # Define machine resources for the batch prediction job
        machine_spec = MachineSpec(machine_type="n1-standard-4") # Example, adjust based on model needs
        dedicated_resources = BatchDedicatedResources(
            machine_spec=machine_spec,
            starting_replica_count=1, # Can be 0 for serverless, or 1+ for dedicated
            max_replica_count=5       # Adjust based on expected load and budget
        )

        # Construct the BatchPredictionJob object
        # NO notification_spec here
        batch_prediction_job_resource = BatchPredictionJob(
            display_name=job_display_name,
            model=model_resource_name,
            input_config=input_config,
            output_config=output_config,
            dedicated_resources=dedicated_resources,
            # Ensure this service account has necessary permissions (Vertex AI User, Storage R/W)
            service_account=f"fire-app-vm-service-account@{gcp_project_id}.iam.gserviceaccount.com",
            # generate_explanation=False, # Set to True if model explanations are needed and configured
            # model_parameters={}, # If your model accepts parameters
        )

        # Create the JobServiceClient with the regional endpoint
        client_options = {"api_endpoint": f"{gcp_region}-aiplatform.googleapis.com"}
        job_service_client = JobServiceClient(client_options=client_options)
        
        parent_path = f"projects/{gcp_project_id}/locations/{gcp_region}"

        # Create the batch prediction job
        created_job_response = job_service_client.create_batch_prediction_job(
            parent=parent_path,
            batch_prediction_job=batch_prediction_job_resource
        )

        _log_json("INFO", "Vertex AI Batch Prediction job creation request sent successfully.",
                           job_display_name=created_job_response.display_name,
                           job_resource_name=created_job_response.name,
                           job_state=str(created_job_response.state)) # Log initial state

    except ValueError as ve:
        _log_json("CRITICAL", f"A configuration or validation error occurred: {str(ve)}", error_type=type(ve).__name__)
        raise # Re-raise to signal Cloud Function failure
    except Exception as e:
        _log_json("CRITICAL", "An unhandled error occurred during pipeline initiation.",
                   error=str(e), error_type=type(e).__name__)
        import traceback
        _log_json("ERROR", "Traceback:", traceback_details=traceback.format_exc())
        raise e # Re-raise to signal Cloud Function failure

    _log_json("INFO", "Pipeline Initiator Cloud Function execution finished.")


# --- Local Testing Entrypoint ---
if __name__ == "__main__":
    print("--- Running local test for Pipeline Initiator Cloud Function ---")

    # Set environment variables for local testing
    # Ensure these are appropriate for your test environment
    os.environ["GCP_PROJECT_ID"] = os.environ.get("GCP_PROJECT_ID", "haryo-kebakaran")
    os.environ["GCP_REGION"] = os.environ.get("GCP_REGION", "asia-southeast2")
    os.environ["FIRMS_API_KEY"] = os.environ.get("FIRMS_API_KEY", "0331973a7ee830ca7f026493faaa367a") # Replace if needed
    os.environ["VERTEX_AI_MODEL_NAME"] = os.environ.get("VERTEX_AI_MODEL_NAME", "dummy_wildfire_detector_v1")
    # Note: VERTEX_NOTIFICATION_PUBSUB_TOPIC_NAME is not used by this script to configure the job.
    # The Pub/Sub topic (e.g., vertex-job-completion-topic) should be the destination for a Cloud Logging Sink.

    if os.environ.get("FIRMS_API_KEY") == "YOUR_FIRMS_API_KEY_PLACEHOLDER": # Example placeholder check
        print("WARNING: FIRMS_API_KEY is set to a placeholder. Real FIRMS data retrieval will likely fail.")

    if GCS_BUCKET_NAME == "fire-app-bucket": # Or your actual bucket name
        print(f"Using configured GCS_BUCKET_NAME: {GCS_BUCKET_NAME} for local test.")
    else:
        print(f"ERROR: GCS_BUCKET_NAME in src/common/config.py ('{GCS_BUCKET_NAME}') might be incorrect for local test.")

    print("Local test will NOT attempt to configure direct Pub/Sub notifications from the Vertex AI Batch Job.")
    print("A Cloud Logging Sink should be configured separately to forward job completion events to Pub/Sub.")
    print(f"Ensure the Vertex AI model '{os.environ['VERTEX_AI_MODEL_NAME']}' exists in project "
          f"'{os.environ['GCP_PROJECT_ID']}' region '{os.environ['GCP_REGION']}'.")
    print("Ensure your local environment is authenticated to GCP (e.g., `gcloud auth application-default login`).")
    print("Ensure 'fire-app-vm-service-account' exists and has necessary roles (Vertex AI User, Storage Object Admin, etc.).")
    print("Ensure Google Earth Engine API is enabled for the project and the service account has 'Earth Engine Data Writer'.")

    # Simulate a Pub/Sub trigger event for local testing
    mock_event_data_str = json.dumps({"message": "Daily trigger from Cloud Scheduler - Local Test (Log Sink for notifications)"})
    mock_event_data_b64 = base64.b64encode(mock_event_data_str.encode('utf-8')).decode('utf-8')
    mock_event = {"data": mock_event_data_b64}

    # Mock the context object
    class MockContext:
        def __init__(self, event_id="test-event-local-initiator-log-sink", timestamp=datetime.now(dt_timezone.utc).isoformat()):
            self.event_id = event_id
            self.timestamp = timestamp
            self.event_type = "google.cloud.pubsub.topic.v1.messagePublished" # Typical for Pub/Sub trigger
            self.resource = {"name": f"projects/{os.environ['GCP_PROJECT_ID']}/topics/your-scheduler-topic", "service": "pubsub.googleapis.com"}

    mock_context_obj = MockContext()

    try:
        pipeline_initiator_cloud_function(mock_event, mock_context_obj)
        print("--- Local test of Pipeline Initiator completed. Check logs, GCS, and Vertex AI for batch job. ---")
        print("--- Remember to set up a Cloud Logging Sink to trigger ResultProcessorCF. ---")
    except ValueError as ve:
        print(f"--- Local test failed due to ValueError: {ve} ---")
    except Exception as e:
        print(f"--- Local test failed with an unexpected error: {type(e).__name__} - {e} ---")
        import traceback
        traceback.print_exc()
</file>

<file path="src/satellite_imagery_acquirer/acquirer.py">
# src/satellite_imagery_acquirer/acquirer.py

import os
import logging
import ee # Google Earth Engine Python API
import json # For structured logging of dicts
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

# Import MONITORED_REGIONS and GCS_BUCKET_NAME from the common config file
# from src.common.config import MONITORED_REGIONS # Not directly used, passed as arg
from src.common.config import GCS_BUCKET_NAME # Used in __main__

# --- Configuration ---
# GEE Project ID for export (Optional, if using custom projects for GEE assets)
# For most use cases, GEE is initialized with your GCP project's credentials.
# GEE_PROJECT_ID = os.environ.get("GCP_PROJECT_ID") # If needed explicitly

# Output directory within GCS bucket for raw imagery
GCS_IMAGE_OUTPUT_PREFIX = "raw_satellite_imagery/"

# Default image collection and filters
# Sentinel-2 Level-2A is a good choice for visible light imagery
# with atmospheric correction.
# See: https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR
SENTINEL2_COLLECTION = "COPERNICUS/S2_SR_HARMONIZED" # Surface Reflectance
CLOUD_COVER_THRESHOLD = 20 # Max percentage of cloud cover allowed

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def _log_json(severity: str, message: str, **kwargs):
    """
    Helper to log structured JSON messages to stdout, which GCP Cloud Logging
    can ingest as structured logs.
    """
    log_entry = {
        "severity": severity.upper(),
        "message": message,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "component": "SatelliteImageryAcquirer",
        **kwargs
    }
    print(json.dumps(log_entry))

class SatelliteImageryAcquirer:
    """
    Component 2: Obtains standard RGB satellite imagery for predefined monitored areas
    using the Google Earth Engine API and exports them to Google Cloud Storage.
    """

    def __init__(self, gcs_bucket_name: str):
        """
        Initializes the SatelliteImageryAcquirer.

        Args:
            gcs_bucket_name (str): The name of the GCS bucket to export images to.
        """
        if not gcs_bucket_name:
            _log_json("CRITICAL", "GCS_BUCKET_NAME is missing or empty. Cannot initialize SatelliteImageryAcquirer.")
            raise ValueError("GCS_BUCKET_NAME must be configured to a valid bucket.")
        self.gcs_bucket_name = gcs_bucket_name

        try:
            ee.Initialize(project=os.environ.get("GCP_PROJECT_ID"))
            _log_json("INFO", "Google Earth Engine initialized successfully.", gcp_project_id=os.environ.get("GCP_PROJECT_ID"))
        except ee.EEException as e:
            _log_json("CRITICAL", "Failed to initialize Google Earth Engine (EEException). "
                                   "Ensure 'earthengine-api' is installed, authentication is set up, "
                                   "and the GCP project has Earth Engine API enabled.", error=str(e))
            raise RuntimeError(f"GEE initialization failed (EEException): {e}")
        except Exception as e:
            _log_json("CRITICAL", "An unexpected error occurred during GEE initialization.", error=str(e))
            raise RuntimeError(f"GEE initialization failed (Unexpected): {e}")


    def _get_latest_composite_image(self, bbox: List[float], date_start: str, date_end: str) -> Optional[ee.Image]:
        """
        Retrieves the latest cloud-filtered Sentinel-2 composite image for a given bbox and date range.

        Args:
            bbox (List[float]): Bounding box [min_lon, min_lat, max_lon, max_lat].
            date_start (str): Start date in 'YYYY-MM-DD' format.
            date_end (str): End date in 'YYYY-MM-DD' format (exclusive).

        Returns:
            Optional[ee.Image]: An Earth Engine Image object or None if no suitable image is found.
        """
        try:
            geometry = ee.Geometry.Rectangle(bbox)

            collection = ee.ImageCollection(SENTINEL2_COLLECTION) \
                .filterDate(date_start, date_end) \
                .filterBounds(geometry) \
                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', CLOUD_COVER_THRESHOLD))

            image = collection.sort('system:time_start', False).sort('CLOUDY_PIXEL_PERCENTAGE').first()

            if image is None or image.bandNames().size().getInfo() == 0:
                _log_json("WARNING", "No suitable image found for the specified region and date range after filtering.",
                          bbox=bbox, date_start=date_start, date_end=date_end, cloud_cover_threshold=CLOUD_COVER_THRESHOLD)
                return None

            _log_json("INFO", "Found a GEE image for processing.",
                      image_id=image.get('system:index').getInfo(),
                      cloud_percentage=image.get('CLOUDY_PIXEL_PERCENTAGE').getInfo(),
                      image_date=datetime.fromtimestamp(image.get('system:time_start').getInfo() / 1000).strftime('%Y-%m-%d'))

            rgb_bands = ['B4', 'B3', 'B2']
            available_bands = image.bandNames().getInfo()
            if not all(band in available_bands for band in rgb_bands):
                _log_json("ERROR", "Selected GEE image is missing one or more required RGB bands (B4, B3, B2).",
                          image_id=image.get('system:index').getInfo(), available_bands=available_bands)
                return None

            rgb_image = image.select(rgb_bands)
            rgb_image_scaled = rgb_image.unitScale(0, 10000).multiply(255).toByte()

            return rgb_image_scaled
        except ee.EEException as e:
            _log_json("ERROR", "GEE error retrieving composite image.",
                      bbox=bbox, date_start=date_start, date_end=date_end, error=str(e))
            return None
        except Exception as e:
             _log_json("ERROR", "Unexpected error retrieving composite image from GEE.",
                      bbox=bbox, date_start=date_start, date_end=date_end, error=str(e), error_type=type(e).__name__)
             return None

    def acquire_and_export_imagery(self, monitored_regions: List[Dict[str, Any]], acquisition_date: Optional[str] = None) -> Dict[str, str]:
        """
        Acquires satellite imagery for each monitored region and exports it to GCS.

        Args:
            monitored_regions (List[Dict[str, Any]]): List of region dictionaries
                                                    with 'id' and 'bbox'.
            acquisition_date (Optional[str]): The target date for imagery in 'YYYY-MM-DD' format.
                                              If None, defaults to yesterday to ensure data availability.

        Returns:
            Dict[str, str]: A dictionary mapping monitored_region_id to its GCS image URI.
                            URIs are for *pending* exports. An empty dict if all fail.
        """
        if acquisition_date:
            try:
                target_date_obj = datetime.strptime(acquisition_date, '%Y-%m-%d')
            except ValueError:
                _log_json("ERROR", "Invalid acquisition_date format. Must be YYYY-MM-DD.", provided_date=acquisition_date)
                return {}
        else:
            target_date_obj = datetime.utcnow() - timedelta(days=1)
            acquisition_date = target_date_obj.strftime('%Y-%m-%d')

        date_start_str = target_date_obj.strftime('%Y-%m-%d')
        date_end_str = (target_date_obj + timedelta(days=1)).strftime('%Y-%m-%d')

        _log_json("INFO", "Starting satellite imagery acquisition.",
                  target_date_for_imagery=date_start_str, gee_date_filter_range=f"[{date_start_str}, {date_end_str})")

        exported_image_uris: Dict[str, str] = {}

        for region in monitored_regions:
            region_id = region["id"]
            region_bbox = region["bbox"]
            image_filename_stem = f"wildfire_imagery_{region_id}_{date_start_str.replace('-', '')}"
            gcs_file_prefix_for_export = f"{GCS_IMAGE_OUTPUT_PREFIX}{image_filename_stem}"
            expected_gcs_image_uri = f"gs://{self.gcs_bucket_name}/{gcs_file_prefix_for_export}.tif"

            _log_json("INFO", "Processing imagery for region.", region_id=region_id, bbox=region_bbox)

            image_to_export = self._get_latest_composite_image(region_bbox, date_start_str, date_end_str)

            if image_to_export is None:
                _log_json("WARNING", f"Skipping imagery export for region '{region_id}' as no suitable GEE image was found.",
                          region_id=region_id)
                continue

            try:
                export_geometry = ee.Geometry.Rectangle(region_bbox).getInfo()['coordinates']

                task = ee.batch.Export.image.toCloudStorage(
                    image=image_to_export,
                    description=f"Export_{image_filename_stem}",
                    bucket=self.gcs_bucket_name,
                    fileNamePrefix=gcs_file_prefix_for_export,
                    scale=10,
                    region=export_geometry,
                    fileFormat='GeoTIFF',
                    # --- MODIFICATION ---
                    # Increased maxPixels to allow larger exports like australia_southeast
                    # Original error indicated ~1.5e10 pixels. Setting to 2e10 for buffer.
                    maxPixels=2e10
                    # --- END MODIFICATION ---
                )
                task.start()
                _log_json("INFO", "GEE export task initiated.",
                          region_id=region_id, task_id=task.id, task_status=task.status(),
                          target_gcs_uri=expected_gcs_image_uri)

                exported_image_uris[region_id] = expected_gcs_image_uri

            except ee.EEException as e:
                _log_json("ERROR", f"GEE export task failed to start for region {region_id} (EEException).",
                          region_id=region_id, error=str(e))
            except Exception as e:
                _log_json("ERROR", f"An unexpected error occurred while initiating GEE export for region {region_id}.",
                          region_id=region_id, error=str(e), error_type=type(e).__name__)

        if not exported_image_uris:
            _log_json("WARNING", "No GEE export tasks were successfully initiated for any region.")
        else:
            _log_json("INFO", "Satellite imagery acquisition process complete. Review GEE Task Manager for export status.",
                      total_tasks_initiated=len(exported_image_uris),
                      initiated_uris=exported_image_uris)
        return exported_image_uris

# --- Example Usage (for local testing) ---
if __name__ == "__main__":
    from src.common.config import MONITORED_REGIONS

    _log_json("INFO", "Starting local test for SatelliteImageryAcquirer.")

    if not os.environ.get("GCP_PROJECT_ID"):
        _log_json("WARNING", "GCP_PROJECT_ID environment variable not set. GEE might use a default project. "
                             "Set it if you encounter permission issues or want to bill to a specific project.")

    if GCS_BUCKET_NAME == "fire-app-bucket": # Replace with your actual bucket if different for testing
        _log_json("INFO", f"Using configured GCS_BUCKET_NAME: {GCS_BUCKET_NAME} for local test.")
    else:
        _log_json("ERROR", f"GCS_BUCKET_NAME in src/common/config.py ('{GCS_BUCKET_NAME}') "
                           "does not match expected 'fire-app-bucket'. Please verify.")
        # exit(1) # Consider exiting if bucket name is critical for the test

    try:
        acquirer = SatelliteImageryAcquirer(gcs_bucket_name=GCS_BUCKET_NAME)
        test_acquisition_date = (datetime.utcnow() - timedelta(days=3)).strftime('%Y-%m-%d')

        _log_json("INFO", f"Attempting to acquire imagery for date: {test_acquisition_date} (local test)")
        image_uris = acquirer.acquire_and_export_imagery(MONITORED_REGIONS, acquisition_date=test_acquisition_date)

        if image_uris:
            _log_json("INFO", "GEE image export tasks initiated (local test). Check GEE Task Manager and GCS bucket.",
                      initiated_uris=image_uris)
        else:
            _log_json("WARNING", "No GEE image export tasks were initiated (local test). Check previous logs for reasons.")

    except ValueError as e:
        _log_json("ERROR", f"Configuration Error during local test: {e}")
    except RuntimeError as e:
        _log_json("ERROR", f"GEE Initialization Error during local test: {e}")
    except Exception as e:
        _log_json("CRITICAL", "An unhandled error occurred during satellite imagery acquisition example (local test).",
                  error=str(e), error_type=type(e).__name__)
</file>

</files>
